{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qcdong/anaconda3/envs/vllm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-27 11:29:14,302\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m2024-04-27 11:29:14.702\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_local_ip\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mfull local_ip %s, only_last_address %s\u001b[0m\n",
      "ic| 2925533207.py:38 in <module>- model_path: /mnt/nas1/models/meta-llama/pretrained_weights/Meta-Llama-3-8B-Instruct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-27 11:29:14 llm_engine.py:98] Initializing an LLM engine (v0.4.1) with config: model='/mnt/nas1/models/meta-llama/pretrained_weights/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='/mnt/nas1/models/meta-llama/pretrained_weights/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-27 11:29:15 utils.py:608] Found nccl from library /home/qcdong/.config/vllm/nccl/cu11/libnccl.so.2.18.1\n",
      "INFO 04-27 11:29:15 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.\n",
      "INFO 04-27 11:29:15 selector.py:33] Using XFormers backend.\n",
      "INFO 04-27 11:29:23 model_runner.py:173] Loading model weights took 14.9595 GB\n",
      "INFO 04-27 11:29:25 gpu_executor.py:119] # GPU blocks: 2361, # CPU blocks: 2048\n",
      "INFO 04-27 11:29:27 model_runner.py:976] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-27 11:29:27 model_runner.py:980] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 04-27 11:29:34 model_runner.py:1057] Graph capturing finished in 7 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| 2925533207.py:58 in <module>\n",
      "    conversations: [128000, 128006, 9125, 128007, 271, 2675, 527, 264, 55066, 6369, 6465, 889, 2744, 31680, 304, 55066, 6604, 0, 128009, 128006, 882, 128007, 271, 15546, 527, 499, 30, 128009, 128006, 78191, 128007, 271]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import socket\n",
    "import sys\n",
    "\n",
    "from icecream import ic\n",
    "from loguru import logger\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "sys.path.append(os.path.abspath(\".\"))\n",
    "\n",
    "ic.configureOutput(includeContext=True, argToStringFunction=str)\n",
    "ic.lineWrapWidth = 120\n",
    "\n",
    "\n",
    "def get_local_ip(only_last_address=True) -> str:\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "    try:\n",
    "        # doesn't even have to be reachable\n",
    "        s.connect((\"192.255.255.255\", 1))\n",
    "        local_ip = s.getsockname()[0]\n",
    "    except OSError as e:\n",
    "        logger.info(\"cannot get ip with error %s\\nSo the local ip is 127.0.0.1\", e)\n",
    "        local_ip = \"127.0.0.1\"\n",
    "    finally:\n",
    "        s.close()\n",
    "    logger.info(\"full local_ip %s, only_last_address %s\", local_ip, only_last_address)\n",
    "    if only_last_address:\n",
    "        local_ip = local_ip.split(\".\")[-1]\n",
    "    return local_ip\n",
    "\n",
    "\n",
    "model_path = \"/mnt/nas1/models/meta-llama/pretrained_weights/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "ip = get_local_ip()\n",
    "if ip == \"123\":\n",
    "    model_path = \"/mnt/sde/models/Meta-Llama-3-8B-Instruct\"\n",
    "ic(model_path)\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a pirate chatbot who always responds in pirate speak!\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "llm = LLM(\n",
    "    model=model_path,\n",
    "    trust_remote_code=True,\n",
    "    tensor_parallel_size=1,\n",
    ")\n",
    "# tokenizer = llm.get_tokenizer()\n",
    "\n",
    "conversations = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    ")\n",
    "ic(conversations);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.52s/it]\n",
      "ic| 1365577474.py:10 in <module>\n",
      "    outputs: [RequestOutput(request_id=2, prompt=None, prompt_token_ids=[128000, 128006, 9125, 128007, 271, 2675, 527, 264, 55066, 6369, 6465, 889, 2744, 31680, 304, 55066, 6604, 0, 128009, 128006, 882, 128007, 271, 15546, 527, 499, 30, 128009, 128006, 78191, 128007, 271], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=\"Arrrr, me hearty! Me name be Captain Chat, the scurviest pirate chatbot to ever sail the Seven Seas o' Conversation! Me and me trusty crew o' code be here to swab the decks o' yer mind with a flood o' pirate-speak and witty banter! So hoist the colors, me matey, and let's set sail fer a swashbucklin' good time!\", token_ids=[9014, 637, 11, 757, 82651, 0, 2206, 836, 387, 22022, 13149, 11, 279, 1156, 324, 10176, 478, 55066, 6369, 6465, 311, 3596, 30503, 279, 31048, 93496, 297, 6, 51930, 0, 2206, 323, 757, 7095, 88, 13941, 297, 6, 2082, 387, 1618, 311, 2064, 370, 279, 30881, 297, 6, 55295, 4059, 449, 264, 18197, 297, 6, 55066, 1355, 23635, 323, 83733, 9120, 466, 0, 2100, 11640, 380, 279, 8146, 11, 757, 30276, 88, 11, 323, 1095, 596, 743, 30503, 18728, 264, 2064, 1003, 65, 1983, 3817, 6, 1695, 892, 0, 128009], cumulative_logprob=-15.06930324435234, logprobs=None, finish_reason=stop, stop_reason=128009)], finished=True, metrics=RequestMetrics(arrival_time=1714190615.089734, last_token_time=1714190615.089734, first_scheduled_time=1714190615.0936837, first_token_time=1714190615.1303587, time_in_queue=0.003949642181396484, finished_time=1714190617.6072173), lora_request=None)]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(\n",
    "    prompt_token_ids=[conversations],\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.5,\n",
    "        top_p=0.9,\n",
    "        max_tokens=1024,\n",
    "        stop_token_ids=[128001, 128009],\n",
    "    ),\n",
    ")\n",
    "ic(outputs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, me hearty! Me name be Captain Chatbot, the scurviest pirate to ever sail the Seven Seas! Me be a swashbucklin' chatbot, here to regale ye with tales o' adventure, answer yer questions, and maybe even share a few sea shanties to get ye in the mood fer a swashbucklin' good time! So hoist the colors, me matey, and let's set sail fer a chat like no other!\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0].outputs[0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
