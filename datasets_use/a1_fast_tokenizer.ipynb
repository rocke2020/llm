{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| 1736770681.py:23 in <module>\n",
      "    encoding2.tokens(): ['Ġ=', 'ĠV', 'alky', 'ria', 'ĠChronicles', 'ĠIII', 'Ġ=', 'Ġ', 'Ċ']\n",
      "    encoding2.word_ids(): [0, 1, 1, 1, 2, 3, 4, 5, 5]\n",
      "    encoding2['offset_mapping']: [(0, 2), (2, 4), (4, 8), (8, 11), (11, 22), (22, 26), (26, 28), (28, 29), (29, 30)]\n",
      "Using pad_token, but it is not set yet.\n",
      "ic| 1736770681.py:27 in <module>- tokenizer.pad_token: None\n",
      "ic| 1736770681.py:30 in <module>- tokenizer.eos_token: <|endoftext|>\n",
      "ic| 1736770681.py:32 in <module>\n",
      "    pad_result: {'input_ids': tensor([[ 3666,  1438,   318, 24286,   391,    11,   290,   314,   670,   379,\n",
      "                         12905,  2667],\n",
      "                        [50256, 50256, 50256,   796,   569, 18354,  7496, 17740,  6711,   796,\n",
      "                           220,   198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "                        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ic| 1736770681.py:36 in <module>\n",
      "    type(encodings): <class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "    type(encodings[0]): <class 'tokenizers.Encoding'>\n",
      "ic| 1736770681.py:37 in <module>\n",
      "    encodings.tokens(): ['My', 'Ġname', 'Ġis', 'ĠSylv', 'ain', ',', 'Ġand', 'ĠI', 'Ġwork', 'Ġat', 'ĠHug', 'ging', 'ĠFace', 'Ġin', 'ĠBrooklyn', 'Ġæ', 'Ī', 'ĳ', 'æĺ¯', 'ä¸Ń', 'åĽ', '½', 'äºº', '81', 'st', '.']\n",
      "    encodings.word_ids(): [0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 9, 10, 11, 12, 13, 13, 13, 13, 13, 13, 13, 13, 14, 15, 16]\n",
      "    encodings[0].word_ids: [0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 9, 10, 11, 12, 13, 13, 13, 13, 13, 13, 13, 13, 14, 15, 16]\n",
      "ic| 1736770681.py:38 in <module>\n",
      "    encodings: {'input_ids': [[3666, 1438, 318, 24286, 391, 11, 290, 314, 670, 379, 12905, 2667, 15399, 287, 12232, 10545, 230, 239, 42468, 40792, 32368, 121, 21689, 6659, 301, 13], [796, 569, 18354, 7496, 17740, 6711, 796, 220, 198]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "ic| 1736770681.py:40 in <module>- start: 10, end: 18, example[start:end]:  Sylvain\n",
      "ic| 1736770681.py:51 in <module>\n",
      "    tokens: ['[CLS]', 'Hello', ',', 'y', \"'\", 'all', '!', '[SEP]', 'How', 'are', 'you', 'doing', '?', '[SEP]']\n",
      "    offsets: [[(0, 0), (0, 5), (5, 6), (7, 8), (8, 9), (9, 12), (12, 13), (0, 0), (0, 3), (4, 7), (8, 11), (12, 17), (17, 18), (0, 0)], [(0, 0), (0, 7), (8, 13), (14, 18), (19, 28), (29, 31), (32, 36), (0, 0), (0, 3), (3, 5), (5, 8), (9, 19), (20, 23), (24, 26), (27, 28), (29, 37), (38, 42), (0, 0)]]\n"
     ]
    }
   ],
   "source": [
    "import os; import psutil; import timeit\n",
    "from datasets import load_dataset\n",
    "import logging\n",
    "from transformers import AutoTokenizer\n",
    "from icecream import ic\n",
    "ic.configureOutput(includeContext=True, argToStringFunction=lambda _: str(_))\n",
    "ic.lineWrapWidth = 120\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, datefmt='%y-%m-%d %H:%M',\n",
    "    format='%(asctime)s %(filename)s %(lineno)d: %(message)s')\n",
    "\n",
    "cache_dir = '/mnt/nas1/huggingface/cache'\n",
    "gpt2_model_name_or_path = '/mnt/nas1/models/gpt2'\n",
    "bert_model_name_or_path = '/mnt/nas1/models/bert-base-cased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(gpt2_model_name_or_path)\n",
    "example = \"My name is Sylvain, and I work at Hugging Face in Brooklyn 我是中国人81st.\"\n",
    "example2 = \" = Valkyria Chronicles III = \\n\"\n",
    "examples = [example, example2]\n",
    "encoding2 = tokenizer(example2, return_offsets_mapping=True)\n",
    "ic(encoding2.tokens(), encoding2.word_ids(), encoding2['offset_mapping'])\n",
    "\n",
    "# default to left padding\n",
    "tokenizer.padding_side = \"left\"\n",
    "ic(tokenizer.pad_token)  # Default pad token is None\n",
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "ic(tokenizer.eos_token)\n",
    "pad_result = tokenizer(examples, padding=True, return_tensors=\"pt\", truncation=True, max_length=12)\n",
    "ic(pad_result)\n",
    "\n",
    "encodings = tokenizer(examples, return_offsets_mapping=False)\n",
    "ic(type(encodings), type(encodings[0]))\n",
    "ic(encodings.tokens(), encodings.word_ids(), encodings[0].word_ids)\n",
    "ic(encodings)\n",
    "start, end = encodings[0].word_to_chars(3)\n",
    "ic(start, end, example[start:end])\n",
    "\n",
    "examples = [[\"Hello, y'all!\", \"How are you doing?\"], \n",
    "            [\"playing video game developed by Sega\", \"Valkyria Chronicles III is a tactical role\"]]\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(bert_model_name_or_path)\n",
    "inputs_with_offsets = tokenizer_bert(\n",
    "    # [example, 'test'], [example2, 'test2'],\n",
    "    examples,\n",
    "    return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "ic(tokens, offsets);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qcdong/anaconda3/envs/pt2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "ic| 4180611331.py:23 in <module>- tokenizer.model_max_length: 1000000000000000019884624838656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/02/2023 14:27:25 - 21 - torch.distributed.nn.jit.instantiator - Created a temporary directory at /tmp/tmpzr47u8nj\n",
      "10/02/2023 14:27:25 - 76 - torch.distributed.nn.jit.instantiator - Writing /tmp/tmpzr47u8nj/_remote_module_non_scriptable.py\n",
      "10/02/2023 14:27:25 - 44 - __main__ - load_wikitext_2_raw_v1\n",
      "10/02/2023 14:27:26 - 62 - __main__ - DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 36718\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4358\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "})\n",
      "10/02/2023 14:27:26 - 95 - __main__ - {'text': ' The game \\'s battle system , the BliTZ system , is carried over directly from Valkyira Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters \\' turns . Each character has a field and distance of movement limited by their Action Gauge . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific \" Potentials \" , skills unique to each character . They are divided into \" Personal Potential \" , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and \" Battle Potentials \" , which are grown throughout the game and always grant boons to a character . To learn Battle Potentials , each character has a unique \" Masters Table \" , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special Abilities that grant them temporary boosts on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without depleting his Action Point gauge , the character Reila can shift into her \" Valkyria Form \" and become invincible , while Imca can target multiple enemy units with her heavy weapon . \\n'}\n",
      "10/02/2023 14:27:26 - 97 - __main__ - 0\n",
      "10/02/2023 14:27:26 - 98 - __main__ - \n",
      "10/02/2023 14:27:26 - 97 - __main__ - 30\n",
      "10/02/2023 14:27:26 - 98 - __main__ -  = Valkyria Chronicles III = \n",
      "\n",
      "10/02/2023 14:27:26 - 97 - __main__ - 0\n",
      "10/02/2023 14:27:26 - 98 - __main__ - \n",
      "10/02/2023 14:27:26 - 97 - __main__ - 706\n",
      "10/02/2023 14:27:26 - 98 - __main__ -  Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \n",
      "\n",
      "10/02/2023 14:27:26 - 100 - __main__ - {'input_ids': [[], [796, 569, 18354, 7496, 17740, 6711, 796, 220, 198], [], [2311, 73, 13090, 645, 569, 18354, 7496, 513, 1058, 791, 47398, 17740, 357, 4960, 1058, 10545, 230, 99, 161, 254, 112, 5641, 44444, 9202, 25084, 24440, 12675, 11839, 18, 837, 6578, 764, 569, 18354, 7496, 286, 262, 30193, 513, 1267, 837, 8811, 6412, 284, 355, 569, 18354, 7496, 17740, 6711, 2354, 2869, 837, 318, 257, 16106, 2597, 2488, 12, 31, 2712, 2008, 983, 4166, 416, 29490, 290, 6343, 13, 44206, 329, 262, 14047, 44685, 764, 28728, 287, 3269, 2813, 287, 2869, 837, 340, 318, 262, 2368, 983, 287, 262, 569, 18354, 7496, 2168, 764, 12645, 278, 262, 976, 21748, 286, 16106, 290, 1103, 2488, 12, 31, 640, 11327, 355, 663, 27677, 837, 262, 1621, 4539, 10730, 284, 262, 717, 983, 290, 5679, 262, 366, 17871, 5321, 366, 837, 257, 23634, 2422, 4326, 7351, 262, 3277, 286, 7096, 544, 1141, 262, 5498, 1898, 6839, 1810, 508, 1620, 3200, 2042, 4560, 290, 389, 46852, 1028, 262, 11773, 4326, 366, 2199, 321, 265, 88, 12552, 366, 764, 220, 198]], 'attention_mask': [[], [1, 1, 1, 1, 1, 1, 1, 1, 1], [], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "10/02/2023 14:27:26 - 111 - __main__ - DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 36718\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 4358\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "})\n",
      "10/02/2023 14:27:26 - 113 - __main__ - 0\n",
      "10/02/2023 14:27:26 - 114 - __main__ - []\n",
      "10/02/2023 14:27:26 - 113 - __main__ - 9\n",
      "10/02/2023 14:27:26 - 114 - __main__ - [796, 569, 18354, 7496, 17740, 6711, 796, 220, 198]\n",
      "10/02/2023 14:27:26 - 113 - __main__ - 0\n",
      "10/02/2023 14:27:26 - 114 - __main__ - []\n",
      "10/02/2023 14:27:26 - 113 - __main__ - 166\n",
      "10/02/2023 14:27:26 - 114 - __main__ - [2311, 73, 13090, 645, 569, 18354, 7496, 513, 1058, 791, 47398, 17740, 357, 4960, 1058, 10545, 230, 99, 161, 254, 112, 5641, 44444, 9202, 25084, 24440, 12675, 11839, 18, 837, 6578, 764, 569, 18354, 7496, 286, 262, 30193, 513, 1267, 837, 8811, 6412, 284, 355, 569, 18354, 7496, 17740, 6711, 2354, 2869, 837, 318, 257, 16106, 2597, 2488, 12, 31, 2712, 2008, 983, 4166, 416, 29490, 290, 6343, 13, 44206, 329, 262, 14047, 44685, 764, 28728, 287, 3269, 2813, 287, 2869, 837, 340, 318, 262, 2368, 983, 287, 262, 569, 18354, 7496, 2168, 764, 12645, 278, 262, 976, 21748, 286, 16106, 290, 1103, 2488, 12, 31, 640, 11327, 355, 663, 27677, 837, 262, 1621, 4539, 10730, 284, 262, 717, 983, 290, 5679, 262, 366, 17871, 5321, 366, 837, 257, 23634, 2422, 4326, 7351, 262, 3277, 286, 7096, 544, 1141, 262, 5498, 1898, 6839, 1810, 508, 1620, 3200, 2042, 4560, 290, 389, 46852, 1028, 262, 11773, 4326, 366, 2199, 321, 265, 88, 12552, 366, 764, 220, 198]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping texts in chunks of 1024 (num_proc=4):   0%|          | 0/36718 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/02/2023 14:27:27 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n",
      "10/02/2023 14:27:27 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n",
      "10/02/2023 14:27:27 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n",
      "10/02/2023 14:27:27 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping texts in chunks of 1024 (num_proc=4):   3%|▎         | 1000/36718 [00:00<00:09, 3753.39 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/02/2023 14:27:27 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n",
      "10/02/2023 14:27:27 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping texts in chunks of 1024 (num_proc=4):   8%|▊         | 3000/36718 [00:00<00:04, 7471.10 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/02/2023 14:27:27 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n",
      "10/02/2023 14:27:27 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n",
      "10/02/2023 14:27:27 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n",
      "10/02/2023 14:27:27 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping texts in chunks of 1024 (num_proc=4):  19%|█▉        | 7000/36718 [00:00<00:02, 11483.15 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/02/2023 14:27:27 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n",
      "10/02/2023 14:27:27 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n",
      "10/02/2023 14:27:27 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n",
      "10/02/2023 14:27:27 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping texts in chunks of 1024 (num_proc=4):  38%|███▊      | 14000/36718 [00:01<00:01, 16646.98 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/02/2023 14:27:28 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n",
      "10/02/2023 14:27:28 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n",
      "10/02/2023 14:27:28 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n",
      "10/02/2023 14:27:28 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping texts in chunks of 1024 (num_proc=4):  44%|████▎     | 16000/36718 [00:01<00:01, 14914.66 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/02/2023 14:27:28 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n",
      "10/02/2023 14:27:28 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping texts in chunks of 1024 (num_proc=4):  49%|████▉     | 18000/36718 [00:01<00:01, 15787.59 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/02/2023 14:27:28 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n",
      "10/02/2023 14:27:28 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n",
      "10/02/2023 14:27:28 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping texts in chunks of 1024 (num_proc=4):  54%|█████▍    | 20000/36718 [00:01<00:01, 14790.73 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/02/2023 14:27:28 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n",
      "10/02/2023 14:27:28 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n",
      "10/02/2023 14:27:28 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n",
      "10/02/2023 14:27:28 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping texts in chunks of 1024 (num_proc=4):  65%|██████▌   | 24000/36718 [00:01<00:00, 16636.94 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/02/2023 14:27:28 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping texts in chunks of 1024 (num_proc=4):  71%|███████   | 26000/36718 [00:01<00:00, 15585.84 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/02/2023 14:27:28 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n",
      "10/02/2023 14:27:28 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n",
      "10/02/2023 14:27:29 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n",
      "10/02/2023 14:27:29 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping texts in chunks of 1024 (num_proc=4):  84%|████████▍ | 31000/36718 [00:02<00:00, 16338.42 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/02/2023 14:27:29 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n",
      "10/02/2023 14:27:29 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n",
      "10/02/2023 14:27:29 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 179\n",
      "10/02/2023 14:27:29 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping texts in chunks of 1024 (num_proc=4):  90%|█████████ | 33179/36718 [00:02<00:00, 16193.12 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/02/2023 14:27:29 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n",
      "10/02/2023 14:27:29 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 179\n",
      "10/02/2023 14:27:29 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping texts in chunks of 1024 (num_proc=4):  99%|█████████▉| 36358/36718 [00:02<00:00, 17953.34 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/02/2023 14:27:29 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping texts in chunks of 1024 (num_proc=4): 100%|██████████| 36718/36718 [00:02<00:00, 13777.95 examples/s]\n",
      "Grouping texts in chunks of 1024 (num_proc=4):   0%|          | 0/4358 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/02/2023 14:27:29 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n",
      "10/02/2023 14:27:29 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n",
      "10/02/2023 14:27:29 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n",
      "10/02/2023 14:27:30 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping texts in chunks of 1024 (num_proc=4):  23%|██▎       | 1000/4358 [00:00<00:01, 3350.62 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/02/2023 14:27:30 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 90\n",
      "10/02/2023 14:27:30 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 90\n",
      "10/02/2023 14:27:30 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 89\n",
      "10/02/2023 14:27:30 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping texts in chunks of 1024 (num_proc=4): 100%|██████████| 4358/4358 [00:00<00:00, 8176.10 examples/s] \n",
      "Grouping texts in chunks of 1024 (num_proc=4):   0%|          | 0/3760 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/02/2023 14:27:30 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 940\n",
      "10/02/2023 14:27:30 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 940\n",
      "10/02/2023 14:27:30 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 940\n",
      "10/02/2023 14:27:30 - 124 - __main__ - 2, ['input_ids', 'attention_mask'], 940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping texts in chunks of 1024 (num_proc=4): 100%|██████████| 3760/3760 [00:00<00:00, 5843.95 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/02/2023 14:27:31 - 146 - __main__ - DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 2314\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 272\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 240\n",
      "    })\n",
      "})\n",
      "10/02/2023 14:27:31 - 147 - __main__ - 2314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import logging, sys, os\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "sys.path.append(os.path.abspath('.'))\n",
    "import logging\n",
    "from icecream import ic\n",
    "ic.configureOutput(includeContext=True, argToStringFunction=lambda _: str(_))\n",
    "ic.lineWrapWidth = 120\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(lineno)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "cache_dir = '/mnt/nas1/huggingface/cache'\n",
    "gpt2_model_name_or_path = '/mnt/nas1/models/gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(gpt2_model_name_or_path)\n",
    "ic(tokenizer.model_max_length)\n",
    "\n",
    "\n",
    "def load_wikitext_2_raw_v1(offline=True, verbose=True):\n",
    "    \"\"\"\n",
    "    DatasetDict({\n",
    "        train: Dataset({\n",
    "            features: ['text'],\n",
    "            num_rows: 36718\n",
    "        })\n",
    "        test: Dataset({\n",
    "            features: ['text'],\n",
    "            num_rows: 4358\n",
    "        })\n",
    "        validation: Dataset({\n",
    "            features: ['text'],\n",
    "            num_rows: 3760\n",
    "        })\n",
    "    })\n",
    "    \"\"\"\n",
    "    wikitext_2_raw_v1_dir = '/mnt/nas1/huggingface/wikitext/wikitext-2-raw-v1'\n",
    "    logger.info('load_wikitext_2_raw_v1')\n",
    "    if offline:\n",
    "        data_files = {\n",
    "            'train': wikitext_2_raw_v1_dir + '/train/' + '0000.parquet',\n",
    "            'test': wikitext_2_raw_v1_dir + '/test/' + '0000.parquet',\n",
    "            'validation': wikitext_2_raw_v1_dir + '/validation/' + '0000.parquet',\n",
    "        }\n",
    "        raw_datasets = load_dataset(\n",
    "            'parquet',\n",
    "            data_files=data_files,\n",
    "            cache_dir=cache_dir,\n",
    "        )\n",
    "    else:\n",
    "        raw_datasets = load_dataset(\n",
    "            'wikitext',\n",
    "            'wikitext-2-raw-v1',\n",
    "            cache_dir=cache_dir,\n",
    "        )\n",
    "    logger.info(raw_datasets)\n",
    "    if verbose:\n",
    "        train_dataset = raw_datasets['train']\n",
    "        count = 0\n",
    "        for item in train_dataset:\n",
    "            logger.info(item)\n",
    "            count += 1\n",
    "            if count > 10:\n",
    "                break\n",
    "    return raw_datasets\n",
    "\n",
    "\n",
    "def test():\n",
    "    \"\"\"  \"\"\"\n",
    "    from transformers.testing_utils import CaptureLogger\n",
    "\n",
    "    # since this will be pickled to avoid _LazyModule error in Hasher force logger loading before tokenize_function\n",
    "    tok_logger = transformers.utils.logging.get_logger(\"transformers.tokenization_utils_base\")\n",
    "    text_column_name = 'text'\n",
    "    raw_datasets = load_wikitext_2_raw_v1(verbose=False)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        with CaptureLogger(tok_logger) as cl:\n",
    "            output = tokenizer(examples[text_column_name])\n",
    "        # clm input could be much much longer than block_size\n",
    "        if \"Token indices sequence length is longer than the\" in cl.out:\n",
    "            tok_logger.warning(\n",
    "                \"^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits\"\n",
    "                \" before being passed to the model.\"\n",
    "            )\n",
    "        return output\n",
    "\n",
    "    train_dataset = raw_datasets['train']\n",
    "    logger.info(train_dataset[10])\n",
    "    for i in range(4):\n",
    "        logger.info(len(raw_datasets['train'][i]['text']))\n",
    "        logger.info(raw_datasets['train'][i]['text'])\n",
    "    output = tokenize_function(train_dataset[:4])\n",
    "    logger.info(output)\n",
    "\n",
    "    column_names = list(raw_datasets[\"train\"].features)\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=4,\n",
    "        remove_columns=column_names,\n",
    "        load_from_cache_file=True,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "    logger.info(tokenized_datasets)\n",
    "    for i in range(4):\n",
    "        logger.info(len(tokenized_datasets['train'][i]['input_ids']))\n",
    "        logger.info(tokenized_datasets['train'][i]['input_ids'])\n",
    "    assert (tokenized_datasets['train'][:4] == output)\n",
    "\n",
    "    block_size = 1024\n",
    "\n",
    "    # Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "    def group_texts(examples):\n",
    "        # Concatenate all texts.\n",
    "        # len(examples), list(examples.keys()), len(examples['input_ids'])\n",
    "        # 2, ['input_ids', 'attention_mask'], 1000 which is batch_size.\n",
    "        logger.info('%s, %s, %s', len(examples), list(examples.keys()), len(examples['input_ids']))\n",
    "        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        # We drop the small remainder, and if the total_length < block_size  we exclude this batch and return an empty dict.\n",
    "        # We could add padding if the model supported it instead of this drop, you can customize this part to your needs.\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "        # Split by chunks of max_len.\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "    \n",
    "    lm_datasets = tokenized_datasets.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        num_proc=4,\n",
    "        # load_from_cache_file=False,\n",
    "        desc=f\"Grouping texts in chunks of {block_size}\",\n",
    "    )\n",
    "    train_dataset = lm_datasets[\"train\"]\n",
    "    logger.info(lm_datasets)\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/01/2023 16:22:16 - INFO - __main__ - Testing 1, 2, 3\n",
      "10/01/2023 16:22:16 - INFO - __main__ - ok\n",
      "10/01/2023 16:22:16 - INFO - __main__ - Testing 1, 2, 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers.testing_utils import CaptureLogger\n",
    "import logging, sys, os\n",
    "\n",
    "msg = \"Testing 1, 2, 3\"\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "with CaptureLogger(logger) as cl:\n",
    "    logger.info(msg)\n",
    "\n",
    "logger.info('ok')\n",
    "logger.info(cl.out)\n",
    "assert cl.out, msg + \"\\n\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
