{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qcdong/anaconda3/envs/pt2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "ic| 1155108691.py:23 in <module>\n",
      "    type(encoding): <class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "    tokenizer.is_fast: True\n",
      "ic| 1155108691.py:24 in <module>\n",
      "    encoding.tokens(): ['Ġ=', 'ĠV', 'alky', 'ria', 'ĠChronicles', 'ĠIII', 'Ġ=', 'Ġ', 'Ċ']\n",
      "    encoding.word_ids(): [0, 1, 1, 1, 2, 3, 4, 5, 5]\n",
      "ic| 1155108691.py:25 in <module>\n",
      "    encoding: {'input_ids': [796, 569, 18354, 7496, 17740, 6711, 796, 220, 198], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 2), (2, 4), (4, 8), (8, 11), (11, 22), (22, 26), (26, 28), (28, 29), (29, 30)]}\n",
      "ic| 1155108691.py:28 in <module>- example[start:end]: d I \n",
      "ic| 1155108691.py:33 in <module>\n",
      "    example: My name is Sylvain, and I work at Hugging Face in Brooklyn 我是中国人81st Street\n",
      "    tokens: ['My', 'Ġname', 'Ġis', 'ĠSylv', 'ain', ',', 'Ġand', 'ĠI', 'Ġwork', 'Ġat', 'ĠHug', 'ging', 'ĠFace', 'Ġin', 'ĠBrooklyn', 'Ġæ', 'Ī', 'ĳ', 'æĺ¯', 'ä¸Ń', 'åĽ', '½', 'äºº', '81', 'st', 'ĠStreet']\n",
      "    offsets: [(0, 2), (2, 7), (7, 10), (10, 15), (15, 18), (18, 19), (19, 23), (23, 25), (25, 30), (30, 33), (33, 37), (37, 41), (41, 46), (46, 49), (49, 58), (58, 60), (59, 60), (59, 60), (60, 61), (61, 62), (62, 63), (62, 63), (63, 64), (64, 66), (66, 68), (68, 75)]\n",
      "ic| 1155108691.py:39 in <module>\n",
      "    tokens: ['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', ',', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in', 'Brooklyn', '[UNK]', '[UNK]', '中', '国', '人', '81', '##st', 'Street', '[SEP]']\n",
      "    offsets: [(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (18, 19), (20, 23), (24, 25), (26, 30), (31, 33), (34, 36), (36, 41), (42, 46), (47, 49), (50, 58), (59, 60), (60, 61), (61, 62), (62, 63), (63, 64), (64, 66), (66, 68), (69, 75), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "import os; import psutil; import timeit\n",
    "from datasets import load_dataset\n",
    "import logging\n",
    "from transformers import AutoTokenizer\n",
    "from icecream import ic\n",
    "ic.configureOutput(includeContext=True, argToStringFunction=lambda _: str(_))\n",
    "ic.lineWrapWidth = 120\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, datefmt='%y-%m-%d %H:%M',\n",
    "    format='%(asctime)s %(filename)s %(lineno)d: %(message)s')\n",
    "\n",
    "cache_dir = '/mnt/nas1/huggingface/cache'\n",
    "gpt2_model_name_or_path = '/mnt/nas1/models/gpt2'\n",
    "bert_model_name_or_path = '/mnt/nas1/models/bert-base-cased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(gpt2_model_name_or_path)\n",
    "example = \"My name is Sylvain, and I work at Hugging Face in Brooklyn 我是中国人81st Street\"\n",
    "example2 = \" = Valkyria Chronicles III = \\n\"\n",
    "encoding = tokenizer(example2, return_offsets_mapping=True)\n",
    "ic(type(encoding), tokenizer.is_fast)\n",
    "ic(encoding.tokens(), encoding.word_ids())\n",
    "ic(encoding)\n",
    "\n",
    "start, end = encoding.word_to_chars(3)\n",
    "ic(example[start:end])\n",
    "\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "ic(example, tokens, offsets)\n",
    "\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(bert_model_name_or_path)\n",
    "inputs_with_offsets = tokenizer_bert(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "ic(tokens, offsets);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qcdong/anaconda3/envs/pt2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "ic| 1337538979.py:23 in <module>- tokenizer.model_max_length: 1000000000000000019884624838656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/01/2023 17:53:18 - INFO - torch.distributed.nn.jit.instantiator - Created a temporary directory at /tmp/tmpemmv_m6u\n",
      "10/01/2023 17:53:18 - INFO - torch.distributed.nn.jit.instantiator - Writing /tmp/tmpemmv_m6u/_remote_module_non_scriptable.py\n",
      "10/01/2023 17:53:18 - INFO - __main__ - load_wikitext_2_raw_v1\n",
      "10/01/2023 17:53:19 - INFO - __main__ - {'text': ' The game \\'s battle system , the BliTZ system , is carried over directly from Valkyira Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters \\' turns . Each character has a field and distance of movement limited by their Action Gauge . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific \" Potentials \" , skills unique to each character . They are divided into \" Personal Potential \" , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and \" Battle Potentials \" , which are grown throughout the game and always grant boons to a character . To learn Battle Potentials , each character has a unique \" Masters Table \" , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special Abilities that grant them temporary boosts on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without depleting his Action Point gauge , the character Reila can shift into her \" Valkyria Form \" and become invincible , while Imca can target multiple enemy units with her heavy weapon . \\n'}\n",
      "10/01/2023 17:53:19 - INFO - __main__ - {'input_ids': [383, 983, 705, 82, 3344, 1080, 837, 262, 1086, 72, 51, 57, 1080, 837, 318, 5281, 625, 3264, 422, 569, 18354, 8704, 17740, 764, 5856, 10566, 837, 1938, 2922, 1123, 4326, 1262, 257, 1353, 2488, 12, 31, 866, 6650, 286, 262, 13480, 3975, 1058, 1752, 257, 2095, 318, 6163, 837, 262, 2137, 6100, 262, 2095, 1088, 262, 13480, 287, 2368, 2488, 12, 31, 1048, 764, 317, 2095, 460, 691, 719, 1752, 583, 2488, 12, 31, 1210, 837, 475, 3435, 460, 307, 7520, 3294, 4962, 379, 262, 10907, 286, 584, 3435, 705, 4962, 764, 5501, 2095, 468, 257, 2214, 290, 5253, 286, 3356, 3614, 416, 511, 7561, 35094, 469, 764, 3205, 284, 5193, 3435, 460, 307, 8686, 284, 257, 2060, 4365, 764, 5856, 11327, 837, 3435, 481, 869, 503, 611, 1223, 4325, 284, 606, 837, 884, 355, 511, 1535, 2173, 357, 6574, 1267, 1972, 1877, 393, 852, 13642, 503, 416, 4472, 3434, 764, 5501, 2095, 468, 2176, 366, 6902, 14817, 366, 837, 4678, 3748, 284, 1123, 2095, 764, 1119, 389, 9086, 656, 366, 15644, 32480, 366, 837, 543, 389, 28690, 4678, 326, 3520, 555, 282, 4400, 4556, 4306, 34756, 416, 262, 1621, 290, 460, 2035, 1037, 393, 43195, 257, 2095, 837, 290, 366, 5838, 6902, 14817, 366, 837, 543, 389, 7334, 3690, 262, 983, 290, 1464, 7264, 1489, 684, 284, 257, 2095, 764, 1675, 2193, 5838, 6902, 14817, 837, 1123, 2095, 468, 257, 3748, 366, 15812, 8655, 366, 837, 257, 10706, 2488, 12, 31, 1912, 5032, 3084, 326, 460, 307, 973, 284, 12831, 290, 2792, 1180, 4678, 764, 26813, 635, 423, 6093, 31447, 326, 7264, 606, 8584, 31822, 319, 262, 13480, 1058, 20642, 460, 15155, 366, 4128, 9455, 366, 290, 1445, 1088, 262, 13480, 1231, 390, 47130, 465, 7561, 6252, 18266, 837, 262, 2095, 797, 10102, 460, 6482, 656, 607, 366, 569, 18354, 7496, 5178, 366, 290, 1716, 46038, 837, 981, 1846, 6888, 460, 2496, 3294, 4472, 4991, 351, 607, 4334, 4282, 764, 220, 198], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "10/01/2023 17:53:19 - INFO - __main__ - {'input_ids': [383, 983, 705, 82, 3344, 1080, 837, 262, 1086, 72, 51, 57, 1080, 837, 318, 5281, 625, 3264, 422, 569, 18354, 8704, 17740, 764, 5856, 10566, 837, 1938, 2922, 1123, 4326, 1262, 257, 1353, 2488, 12, 31, 866, 6650, 286, 262, 13480, 3975, 1058, 1752, 257, 2095, 318, 6163, 837, 262, 2137, 6100, 262, 2095, 1088, 262, 13480, 287, 2368, 2488, 12, 31, 1048, 764, 317, 2095, 460, 691, 719, 1752, 583, 2488, 12, 31, 1210, 837, 475, 3435, 460, 307, 7520, 3294, 4962, 379, 262, 10907, 286, 584, 3435, 705, 4962, 764, 5501, 2095, 468, 257, 2214, 290, 5253, 286, 3356, 3614, 416, 511, 7561, 35094, 469, 764, 3205, 284, 5193, 3435, 460, 307, 8686, 284, 257, 2060, 4365, 764, 5856, 11327, 837, 3435, 481, 869, 503, 611, 1223, 4325, 284, 606, 837, 884, 355, 511, 1535, 2173, 357, 6574, 1267, 1972, 1877, 393, 852, 13642, 503, 416, 4472, 3434, 764, 5501, 2095, 468, 2176, 366, 6902, 14817, 366, 837, 4678, 3748, 284, 1123, 2095, 764, 1119, 389, 9086, 656, 366, 15644, 32480, 366, 837, 543, 389, 28690, 4678, 326, 3520, 555, 282, 4400, 4556, 4306, 34756, 416, 262, 1621, 290, 460, 2035, 1037, 393, 43195, 257, 2095, 837, 290, 366, 5838, 6902, 14817, 366, 837, 543, 389, 7334, 3690, 262, 983, 290, 1464, 7264, 1489, 684, 284, 257, 2095, 764, 1675, 2193, 5838, 6902, 14817, 837, 1123, 2095, 468, 257, 3748, 366, 15812, 8655, 366, 837, 257, 10706, 2488, 12, 31, 1912, 5032, 3084, 326, 460, 307, 973, 284, 12831, 290, 2792, 1180, 4678, 764, 26813, 635, 423, 6093, 31447, 326, 7264, 606, 8584, 31822, 319, 262, 13480, 1058, 20642, 460, 15155, 366, 4128, 9455, 366, 290, 1445, 1088, 262, 13480, 1231, 390, 47130, 465, 7561, 6252, 18266, 837, 262, 2095, 797, 10102, 460, 6482, 656, 607, 366, 569, 18354, 7496, 5178, 366, 290, 1716, 46038, 837, 981, 1846, 6888, 460, 2496, 3294, 4472, 4991, 351, 607, 4334, 4282, 764, 220, 198], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/qcdong/codes/tiny-gpt/datasets_use/a1_fast_tokenizer.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bremote125/home/qcdong/codes/tiny-gpt/datasets_use/a1_fast_tokenizer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=129'>130</a>\u001b[0m     logger\u001b[39m.\u001b[39minfo(out)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bremote125/home/qcdong/codes/tiny-gpt/datasets_use/a1_fast_tokenizer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=130'>131</a>\u001b[0m     \u001b[39m# lm_datasets = tokenized_datasets.map(\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bremote125/home/qcdong/codes/tiny-gpt/datasets_use/a1_fast_tokenizer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=131'>132</a>\u001b[0m     \u001b[39m#     group_texts,\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bremote125/home/qcdong/codes/tiny-gpt/datasets_use/a1_fast_tokenizer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=132'>133</a>\u001b[0m     \u001b[39m#     batched=True,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bremote125/home/qcdong/codes/tiny-gpt/datasets_use/a1_fast_tokenizer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=135'>136</a>\u001b[0m     \u001b[39m#     desc=f\"Grouping texts in chunks of {block_size}\",\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bremote125/home/qcdong/codes/tiny-gpt/datasets_use/a1_fast_tokenizer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=136'>137</a>\u001b[0m     \u001b[39m# )\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bremote125/home/qcdong/codes/tiny-gpt/datasets_use/a1_fast_tokenizer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=138'>139</a>\u001b[0m test()\n",
      "\u001b[1;32m/home/qcdong/codes/tiny-gpt/datasets_use/a1_fast_tokenizer.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bremote125/home/qcdong/codes/tiny-gpt/datasets_use/a1_fast_tokenizer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=125'>126</a>\u001b[0m     result[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m result[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bremote125/home/qcdong/codes/tiny-gpt/datasets_use/a1_fast_tokenizer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=126'>127</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bremote125/home/qcdong/codes/tiny-gpt/datasets_use/a1_fast_tokenizer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=128'>129</a>\u001b[0m out \u001b[39m=\u001b[39m group_texts(tokenized_datasets[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m10\u001b[39;49m])\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bremote125/home/qcdong/codes/tiny-gpt/datasets_use/a1_fast_tokenizer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=129'>130</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(out)\n",
      "\u001b[1;32m/home/qcdong/codes/tiny-gpt/datasets_use/a1_fast_tokenizer.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bremote125/home/qcdong/codes/tiny-gpt/datasets_use/a1_fast_tokenizer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=113'>114</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgroup_texts\u001b[39m(examples):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bremote125/home/qcdong/codes/tiny-gpt/datasets_use/a1_fast_tokenizer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=114'>115</a>\u001b[0m     \u001b[39m# Concatenate all texts.\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bremote125/home/qcdong/codes/tiny-gpt/datasets_use/a1_fast_tokenizer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=115'>116</a>\u001b[0m     concatenated_examples \u001b[39m=\u001b[39m {k: \u001b[39mlist\u001b[39m(chain(\u001b[39m*\u001b[39mexamples[k])) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m examples\u001b[39m.\u001b[39mkeys()}\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bremote125/home/qcdong/codes/tiny-gpt/datasets_use/a1_fast_tokenizer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=116'>117</a>\u001b[0m     total_length \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(concatenated_examples[\u001b[39mlist\u001b[39m(examples\u001b[39m.\u001b[39mkeys())[\u001b[39m0\u001b[39m]])\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bremote125/home/qcdong/codes/tiny-gpt/datasets_use/a1_fast_tokenizer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=117'>118</a>\u001b[0m     \u001b[39m# We drop the small remainder, and if the total_length < block_size  we exclude this batch and return an empty dict.\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bremote125/home/qcdong/codes/tiny-gpt/datasets_use/a1_fast_tokenizer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=118'>119</a>\u001b[0m     \u001b[39m# We could add padding if the model supported it instead of this drop, you can customize this part to your needs.\u001b[39;00m\n",
      "\u001b[1;32m/home/qcdong/codes/tiny-gpt/datasets_use/a1_fast_tokenizer.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bremote125/home/qcdong/codes/tiny-gpt/datasets_use/a1_fast_tokenizer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=113'>114</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgroup_texts\u001b[39m(examples):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bremote125/home/qcdong/codes/tiny-gpt/datasets_use/a1_fast_tokenizer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=114'>115</a>\u001b[0m     \u001b[39m# Concatenate all texts.\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bremote125/home/qcdong/codes/tiny-gpt/datasets_use/a1_fast_tokenizer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=115'>116</a>\u001b[0m     concatenated_examples \u001b[39m=\u001b[39m {k: \u001b[39mlist\u001b[39;49m(chain(\u001b[39m*\u001b[39;49mexamples[k])) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m examples\u001b[39m.\u001b[39mkeys()}\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bremote125/home/qcdong/codes/tiny-gpt/datasets_use/a1_fast_tokenizer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=116'>117</a>\u001b[0m     total_length \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(concatenated_examples[\u001b[39mlist\u001b[39m(examples\u001b[39m.\u001b[39mkeys())[\u001b[39m0\u001b[39m]])\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bremote125/home/qcdong/codes/tiny-gpt/datasets_use/a1_fast_tokenizer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=117'>118</a>\u001b[0m     \u001b[39m# We drop the small remainder, and if the total_length < block_size  we exclude this batch and return an empty dict.\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bremote125/home/qcdong/codes/tiny-gpt/datasets_use/a1_fast_tokenizer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=118'>119</a>\u001b[0m     \u001b[39m# We could add padding if the model supported it instead of this drop, you can customize this part to your needs.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import logging, sys, os\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "sys.path.append(os.path.abspath('.'))\n",
    "import logging\n",
    "from icecream import ic\n",
    "ic.configureOutput(includeContext=True, argToStringFunction=lambda _: str(_))\n",
    "ic.lineWrapWidth = 120\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "cache_dir = '/mnt/nas1/huggingface/cache'\n",
    "gpt2_model_name_or_path = '/mnt/nas1/models/gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(gpt2_model_name_or_path)\n",
    "ic(tokenizer.model_max_length)\n",
    "\n",
    "\n",
    "def load_wikitext_2_raw_v1(offline=True, verbose=True):\n",
    "    \"\"\"  \n",
    "    DatasetDict({\n",
    "        train: Dataset({\n",
    "            features: ['text'],\n",
    "            num_rows: 36718\n",
    "        })\n",
    "        test: Dataset({\n",
    "            features: ['text'],\n",
    "            num_rows: 4358\n",
    "        })\n",
    "        validation: Dataset({\n",
    "            features: ['text'],\n",
    "            num_rows: 3760\n",
    "        })\n",
    "    })    \n",
    "    \"\"\"\n",
    "    wikitext_2_raw_v1_dir = '/mnt/nas1/huggingface/wikitext/wikitext-2-raw-v1'\n",
    "    logger.info('load_wikitext_2_raw_v1')\n",
    "    if offline:\n",
    "        data_files = {\n",
    "            'train': wikitext_2_raw_v1_dir + '/train/' + '0000.parquet',\n",
    "            'test': wikitext_2_raw_v1_dir + '/test/' + '0000.parquet',\n",
    "            'validation': wikitext_2_raw_v1_dir + '/validation/' + '0000.parquet',\n",
    "        }\n",
    "        raw_datasets = load_dataset(\n",
    "            'parquet',\n",
    "            data_files=data_files,\n",
    "            cache_dir=cache_dir,\n",
    "        )\n",
    "    else:\n",
    "        raw_datasets = load_dataset(\n",
    "            'wikitext',\n",
    "            'wikitext-2-raw-v1',\n",
    "            cache_dir=cache_dir,\n",
    "        )\n",
    "    if verbose:\n",
    "        logger.info(raw_datasets)\n",
    "        train_dataset = raw_datasets['train']\n",
    "        count = 0\n",
    "        for item in train_dataset:\n",
    "            logger.info(item)\n",
    "            count += 1\n",
    "            if count > 10:\n",
    "                break\n",
    "    return raw_datasets\n",
    "\n",
    "\n",
    "def test():\n",
    "    \"\"\"  \"\"\"\n",
    "    from transformers.testing_utils import CaptureLogger\n",
    "\n",
    "    # since this will be pickled to avoid _LazyModule error in Hasher force logger loading before tokenize_function\n",
    "    tok_logger = transformers.utils.logging.get_logger(\"transformers.tokenization_utils_base\")\n",
    "    text_column_name = 'text'\n",
    "    raw_datasets = load_wikitext_2_raw_v1(verbose=False)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        with CaptureLogger(tok_logger) as cl:\n",
    "            output = tokenizer(examples[text_column_name])\n",
    "        # clm input could be much much longer than block_size\n",
    "        if \"Token indices sequence length is longer than the\" in cl.out:\n",
    "            tok_logger.warning(\n",
    "                \"^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits\"\n",
    "                \" before being passed to the model.\"\n",
    "            )\n",
    "        return output\n",
    "    \n",
    "    train_dataset = raw_datasets['train']\n",
    "    logger.info(train_dataset[10])\n",
    "    output = tokenize_function(train_dataset[10])\n",
    "    logger.info(output)\n",
    "\n",
    "    column_names = list(raw_datasets[\"train\"].features)\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=4,\n",
    "        remove_columns=column_names,\n",
    "        load_from_cache_file=True,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "    # logger.info(tokenized_datasets['train'])\n",
    "    logger.info(tokenized_datasets['train'][10])\n",
    "\n",
    "    block_size = 1024\n",
    "\n",
    "    # Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "    def group_texts(examples):\n",
    "        # Concatenate all texts.\n",
    "        logger.info(examples)\n",
    "        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        # We drop the small remainder, and if the total_length < block_size  we exclude this batch and return an empty dict.\n",
    "        # We could add padding if the model supported it instead of this drop, you can customize this part to your needs.\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "        # Split by chunks of max_len.\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "    \n",
    "    out = group_texts(tokenized_datasets['train'][10])\n",
    "    logger.info(out)\n",
    "    # lm_datasets = tokenized_datasets.map(\n",
    "    #     group_texts,\n",
    "    #     batched=True,\n",
    "    #     num_proc=4,\n",
    "    #     load_from_cache_file=True,\n",
    "    #     desc=f\"Grouping texts in chunks of {block_size}\",\n",
    "    # )\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/01/2023 16:22:16 - INFO - __main__ - Testing 1, 2, 3\n",
      "10/01/2023 16:22:16 - INFO - __main__ - ok\n",
      "10/01/2023 16:22:16 - INFO - __main__ - Testing 1, 2, 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers.testing_utils import CaptureLogger\n",
    "import logging, sys, os\n",
    "\n",
    "msg = \"Testing 1, 2, 3\"\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "with CaptureLogger(logger) as cl:\n",
    "    logger.info(msg)\n",
    "\n",
    "logger.info('ok')\n",
    "logger.info(cl.out)\n",
    "assert cl.out, msg + \"\\n\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
