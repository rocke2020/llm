{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load input text sentences\n",
    "from pandas import read_json, Series\n",
    "CTD_RE_V1 = read_json('../label_studio/export/CTD_RE_v1.json').set_index('id')\n",
    "sentences = Series(data = [row['text'] for row in CTD_RE_V1.data], index=CTD_RE_V1.index)\n",
    "\n",
    "# load test sample ids\n",
    "from csv import reader\n",
    "with open(\"test_output_2000/sampled_test_ids.csv\", \"r\") as file:\n",
    "    sampled_test_ids = list(map(int, list(reader(file, delimiter=\",\"))[0]))\n",
    "    file.close()\n",
    "\n",
    "# load base model\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "#base_model_id = \"/mnt/sdc/llama_hf/llama-2-7b-hf\"\n",
    "base_model_id = \"/home/qyfeng/llama_hf/Meta-Llama-3-8B\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  # Llama 2 7b, same as finetuning\n",
    "    quantization_config=bnb_config,  # Same quantization config as finetuning\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "ft_model_260 = PeftModel.from_pretrained(base_model, \"finetuned_models/llama3-8b-CTD_RE_V1-finetune-r_8_la_32-prompt_v3-random_2000-claude_langchain/checkpoint-260\")\n",
    "ft_model_450 = PeftModel.from_pretrained(base_model, \"finetuned_models/llama3-8b-CTD_RE_V1-finetune-r_8_la_32-prompt_v3-random_2000-claude_langchain/checkpoint-450\")\n",
    "ft_model_670 = PeftModel.from_pretrained(base_model, \"finetuned_models/llama3-8b-CTD_RE_V1-finetune-r_8_la_32-prompt_v3-random_2000-claude_langchain/checkpoint-670\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "ft_model_260 = PeftModel.from_pretrained(base_model, \"finetuned_models/llama3-8b-CTD_RE_V1-finetune-r_8_la_32-prompt_v3-random_2000-claude_langchain/checkpoint-260\")\n",
    "ft_model_450 = PeftModel.from_pretrained(base_model, \"finetuned_models/llama3-8b-CTD_RE_V1-finetune-r_8_la_32-prompt_v3-random_2000-claude_langchain/checkpoint-450\")\n",
    "ft_model_670 = PeftModel.from_pretrained(base_model, \"finetuned_models/llama3-8b-CTD_RE_V1-finetune-r_8_la_32-prompt_v3-random_2000-claude_langchain/checkpoint-670\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load helper functions\n",
    "def format_relation(relations):\n",
    "    relation_str = \"\"\n",
    "    for relation in relations:\n",
    "        relation_str += (\"((\" + relation['subject_entity']['entity_name'] + \", \" + relation['subject_entity']['entity_type']+ \"), \" +\n",
    "                            relation['relation_phrase']+ \", \" +\n",
    "                            \"(\" + relation['object_entity']['entity_name'] + \", \" + relation['object_entity']['entity_type']+ \"))\" + \"; \")\n",
    "        \n",
    "    return relation_str\n",
    "def formatting_func_v3(data_point):\n",
    "    full_prompt = f\"\"\"Given an input text sentence, extract fact relations.\n",
    "    Each fact relation describes a scientific observation or hypothesis and is in the format of a triple connecting two entities via a relation phrase: (subject_entity, relation_phrase, object_entity).\n",
    "    Each subject_entity or object_entity is a chemical compound or gene/protein and is in the format of a 2-tuple: (entity_name, entity_type). Depending on the type of the entity, the entity_type must be one of ['Chemical', 'Gene/Protein'].\n",
    "    The relation_phrase must be one of the following: ['increases', 'decreases', 'affects', 'binds'].\n",
    "    The extracted relations should be a semicolon-separated list of relations in the format of triples: ((entity_name, entity_type), relation_phrase, (entity_name, entity_type)).\n",
    "    \n",
    "    ### Input sentence:\n",
    "    {data_point[\"input_sentence\"]}\n",
    "\n",
    "    ### Extracted relations:\n",
    "    {format_relation(data_point[\"relations\"])}\n",
    "    \"\"\"\n",
    "    return full_prompt\n",
    "\n",
    "def generate_text(input_sentence, model):\n",
    "    eval_example = {'input_sentence': input_sentence, 'relations': []}\n",
    "    eval_prompt = formatting_func_v3(eval_example)\n",
    "    model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output_text = eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=512)[0], skip_special_tokens=True)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for task_id in tqdm(sampled_test_ids):\n",
    "    with open('test_output_2000/claude/llama3-8b-CTD_RE_V1-finetune-r_8_la_32-prompt_v3-random_2000-claude_langchain-checkpoint-260/'+str(task_id)+'.txt', \"w\") as outfile:\n",
    "        outfile.write(generate_text(sentences[task_id], ft_model_260))\n",
    "        outfile.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
