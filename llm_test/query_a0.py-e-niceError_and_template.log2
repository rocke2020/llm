The error shows the key running process, nice!

You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
llm.is_chat_model = False
query_engine.get_prompts() = {'response_synthesizer:text_qa_template': SelectorPromptTemplate(metadata={'prompt_type': <PromptType.QUESTION_ANSWER: 'text_qa'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings={}, function_mappings={}, default_template=PromptTemplate(metadata={'prompt_type': <PromptType.QUESTION_ANSWER: 'text_qa'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template='Context information is below.\n---------------------\n{context_str}\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: {query_str}\nAnswer: '), conditionals=[(<function is_chat_model at 0x7fa6b82252d0>, ChatPromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, message_templates=[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content="You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.", additional_kwargs={}), ChatMessage(role=<MessageRole.USER: 'user'>, content='Context information is below.\n---------------------\n{context_str}\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: {query_str}\nAnswer: ', additional_kwargs={})]))]), 'response_synthesizer:refine_template': SelectorPromptTemplate(metadata={'prompt_type': <PromptType.REFINE: 'refine'>}, template_vars=['query_str', 'existing_answer', 'context_msg'], kwargs={}, output_parser=None, template_var_mappings={}, function_mappings={}, default_template=PromptTemplate(metadata={'prompt_type': <PromptType.REFINE: 'refine'>}, template_vars=['query_str', 'existing_answer', 'context_msg'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template="The original query is as follows: {query_str}\nWe have provided an existing answer: {existing_answer}\nWe have the opportunity to refine the existing answer (only if needed) with some more context below.\n------------\n{context_msg}\n------------\nGiven the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\nRefined Answer: "), conditionals=[(<function is_chat_model at 0x7fa6b82252d0>, ChatPromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['context_msg', 'query_str', 'existing_answer'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, message_templates=[ChatMessage(role=<MessageRole.USER: 'user'>, content="You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.\nNew Context: {context_msg}\nQuery: {query_str}\nOriginal Answer: {existing_answer}\nNew Answer: ", additional_kwargs={})]))])}

Batches:   0%|          | 0/1 [00:00<?, ?it/s]
Batches: 100%|██████████| 1/1 [00:00<00:00, 70.30it/s]
Traceback (most recent call last):
  File "/home/qcdong/codes/llamaIndex/llm_test/query_a0.py", line 68, in <module>
    response = query_engine.query(QUESTION)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py", line 146, in wrapper
    self.span_drop(id_=id_, bound_args=bound_args, instance=instance, err=e)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py", line 98, in span_drop
    h.span_drop(
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/instrumentation/span_handlers/base.py", line 77, in span_drop
    span = self.prepare_to_drop_span(
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/instrumentation/span_handlers/null.py", line 71, in prepare_to_drop_span
    raise err
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py", line 144, in wrapper
    result = func(*args, **kwargs)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/base/base_query_engine.py", line 51, in query
    query_result = self._query(str_or_query_bundle)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py", line 146, in wrapper
    self.span_drop(id_=id_, bound_args=bound_args, instance=instance, err=e)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py", line 98, in span_drop
    h.span_drop(
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/instrumentation/span_handlers/base.py", line 77, in span_drop
    span = self.prepare_to_drop_span(
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/instrumentation/span_handlers/null.py", line 71, in prepare_to_drop_span
    raise err
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py", line 144, in wrapper
    result = func(*args, **kwargs)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 190, in _query
    response = self._response_synthesizer.synthesize(
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py", line 146, in wrapper
    self.span_drop(id_=id_, bound_args=bound_args, instance=instance, err=e)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py", line 98, in span_drop
    h.span_drop(
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/instrumentation/span_handlers/base.py", line 77, in span_drop
    span = self.prepare_to_drop_span(
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/instrumentation/span_handlers/null.py", line 71, in prepare_to_drop_span
    raise err
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py", line 144, in wrapper
    result = func(*args, **kwargs)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/response_synthesizers/base.py", line 228, in synthesize
    response_str = self.get_response(
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py", line 146, in wrapper
    self.span_drop(id_=id_, bound_args=bound_args, instance=instance, err=e)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py", line 98, in span_drop
    h.span_drop(
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/instrumentation/span_handlers/base.py", line 77, in span_drop
    span = self.prepare_to_drop_span(
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/instrumentation/span_handlers/null.py", line 71, in prepare_to_drop_span
    raise err
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py", line 144, in wrapper
    result = func(*args, **kwargs)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py", line 43, in get_response
    return super().get_response(
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py", line 146, in wrapper
    self.span_drop(id_=id_, bound_args=bound_args, instance=instance, err=e)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py", line 98, in span_drop
    h.span_drop(
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/instrumentation/span_handlers/base.py", line 77, in span_drop
    span = self.prepare_to_drop_span(
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/instrumentation/span_handlers/null.py", line 71, in prepare_to_drop_span
    raise err
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py", line 144, in wrapper
    result = func(*args, **kwargs)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/response_synthesizers/refine.py", line 181, in get_response
    response = self._give_response_single(
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/response_synthesizers/refine.py", line 236, in _give_response_single
    program(
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/response_synthesizers/refine.py", line 84, in __call__
    answer = self._llm.predict(
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py", line 146, in wrapper
    self.span_drop(id_=id_, bound_args=bound_args, instance=instance, err=e)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py", line 98, in span_drop
    h.span_drop(
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/instrumentation/span_handlers/base.py", line 77, in span_drop
    span = self.prepare_to_drop_span(
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/instrumentation/span_handlers/null.py", line 71, in prepare_to_drop_span
    raise err
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py", line 144, in wrapper
    result = func(*args, **kwargs)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/llms/llm.py", line 417, in predict
    response = self.complete(formatted_prompt, formatted=True)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/core/llms/callbacks.py", line 294, in wrapped_llm_predict
    f_return_val = f(_self, *args, **kwargs)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/llama_index/llms/huggingface/base.py", line 281, in complete
    tokens = self._model.generate(
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/transformers/generation/utils.py", line 1527, in generate
    result = self._greedy_search(
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/transformers/generation/utils.py", line 2411, in _greedy_search
    outputs = self(
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 1157, in forward
    outputs = self.model(
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 1042, in forward
    layer_outputs = decoder_layer(
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 770, in forward
    hidden_states = self.mlp(hidden_states)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 179, in forward
    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/bitsandbytes/nn/modules.py", line 687, in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py", line 562, in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/torch/autograd/function.py", line 553, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py", line 387, in forward
    CA[:, state.idx.long()] = 0
RuntimeError: CUDA error: unspecified launch failure
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
