{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from llama_index.core.program import LLMTextCompletionProgram\n",
    "\n",
    "class taskB(BaseModel):\n",
    "    peptideName: str\n",
    "    peptideSequences: str\n",
    "    relevantContent: str\n",
    "\n",
    "class information(BaseModel):\n",
    "    taskA: str\n",
    "    taskB: List[taskB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://192.168.1.254:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://192.168.1.254:7890\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "openchat_35_0106 = \"/mnt/nas1/models/openchat-3.5-0106\"\n",
    "selected_model = openchat_35_0106\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an AI assistant that answers questions in a friendly manner, based on the given source documents. Here are some rules you always follow:\n",
    "- Generate human readable output, avoid creating output with gibberish text.\n",
    "- Generate only the requested output, don't include any other language before or after the requested output.\n",
    "- Never say thank you, that you are happy to help, that you are an AI agent, etc. Just answer directly.\n",
    "- Generate professional language typically used in business documents in North America.\n",
    "- Never generate offensive or foul language.\n",
    "\"\"\"\n",
    "\n",
    "query_wrapper_prompt = PromptTemplate(\n",
    "    \"[INST]<<SYS>>\\n\" + SYSTEM_PROMPT + \"<</SYS>>\\n\\n{query_str}[/INST] \"\n",
    ")\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name = selected_model,\n",
    "    tokenizer_name=selected_model,\n",
    "    context_window = 8192,\n",
    "    max_new_tokens = 4096,\n",
    "    generate_kwargs = {\"temperature\": 0.0, \"do_sample\": False},\n",
    "    query_wrapper_prompt = query_wrapper_prompt,\n",
    "    device_map = \"auto\",\n",
    ")\n",
    "\n",
    "from llama_index.core import Settings\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.llama_api import LlamaAPI\n",
    "\n",
    "llama_api_key = 'LL-SxQGxNT1jWMZyQJgOQPNuOoUlAz1UhSkTqlYEicFUaPRajxavvY7jBbvYmN53ncI'\n",
    "llm = LlamaAPI(api_key = llama_api_key, model = 'llama-7b-chat', temperature = 0.0)\n",
    "from llama_index.core import Settings\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_str = \"\"\"\\\n",
    "The answer are based solely on the information provided in the given context and do not rely on any prior knowledge or assumptions. \\\n",
    "The given context is : {contexts}. \\\n",
    "Please only use the given content to complete the following tasks. \\\n",
    "Task Aï¼šonly Use YES or NO to answer whether the given content contains an anti-inflammatory peptide. Don't have redundant content. \\\n",
    "Task B: List the names, sequences, and related content of all anti-inflammatory peptides in the following format: [peptideName:'', peptideSequence:'', relatedContent:'']. \\\n",
    "Generate some example information, with the task A and task B. \\\n",
    "\"\"\"\n",
    "\n",
    "from llama_index.core.output_parsers import PydanticOutputParser\n",
    "\n",
    "program = LLMTextCompletionProgram.from_defaults(\n",
    "    output_parser=PydanticOutputParser(output_cls=information),\n",
    "    prompt_template_str=prompt_template_str,\n",
    "    verbose=True,\n",
    ")\n",
    "output = program(contexts = \"Boosting transcorneal permeability and pharmacological activity of drug poses a great challenge in the field of ocular drug delivery. In the present study, we propose a drug-peptide supramolecular hydrogel based on anti-inflammatory drug, dexamethasone (Dex), and Arg-Gly-Asp (RGD) motif for boosting transcorneal permeability and pharmacological activity via the ligand-receptor interaction. The drug-peptide (Dex-SA-RGD/RGE) supramolecular hydrogel comprised of uniform nanotube architecture formed spontaneously in phosphate buffered saline (PBS, pH\\u00a0=\\u00a07.4) without external stimuli. Upon storage at 4\\u00a0\\u00b0C, 25\\u00a0\\u00b0C, and 37\\u00a0\\u00b0C for 70 days, Dex-SA-RGD in hydrogel did not undergo significant hydrolysis, suggesting great long-term stability. In comparison to Dex-SA-RGE, Dex-SA-RGD exhibited a more potent in vitro anti-inflammatory efficacy in lipopolysaccharide (LPS)-activated RAW 264.7 macrophages via the inhibition of nuclear factor \\u043aB (NF-\\u03baB) signal pathway. More importantly, using drug-peptide supramolecular hydrogel labeled with 7-nitro-2,1,3-benzoxadiazole (NBD), the Dex-SA-K(NBD)RGD showed increased performance in terms of integrin targeting and cellular uptake compared to Dex-SA-K(NBD)RGE, as revealed by cellular uptake assay. On topical instillation in rabbit's eye, the proposed Dex-SA-K(NBD)RGD could effectively enhance the transcorneal distribution and permeability with respect to the Dex-SA-K(NBD)RGE. Overall, our findings demonstrate the performance of the ligand-receptor interaction for boosting transcorneal permeability and pharmacological activity of drug.\")\n",
    "# output = program(context = '{}'.format(''.join(['Content {}: {}'.format(index, node.text) for index,node in enumerate(response_nodes)])))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1491\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('/mnt/nas1/patent_data/anti-inflammation_peptide/parsed_pmc.json', 'r') as f:\n",
    "    js_file = json.load(f)\n",
    "print(len(js_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1486\n"
     ]
    }
   ],
   "source": [
    "list_info = []\n",
    "for item in js_file:\n",
    "    information = {\"pmc\":\"\", \"pmid\":\"\", \"paragraph\":[]}\n",
    "\n",
    "    if item.get('pmc') == None:\n",
    "        continue\n",
    "    information['pmc'] = item['pmc']\n",
    "    information['pmid'] = item['pmid']\n",
    "\n",
    "    for sec in item['paragraph']:\n",
    "        paragraph_dict = {\"section\":\"\", \"text\":\"\"}\n",
    "        paragraph_dict['section'] = sec['root_section']\n",
    "\n",
    "        if sec['root_section'] == sec['father_section'] and sec['father_section'] == sec['section']:\n",
    "            paragraph_dict['text'] = sec['text']\n",
    "        elif sec['section'] != sec['father_section']:\n",
    "            paragraph_dict['text'] = \"{}: {}\".format(sec['section'], sec['text'])\n",
    "        information['paragraph'].append(paragraph_dict)   \n",
    "    list_info.append(information)\n",
    "print(len(list_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/nas1/patent_data/anti-inflammation_peptide/parsed_pmc_merge.json', 'w') as file:\n",
    "    json.dump(list_info, file, indent = 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
