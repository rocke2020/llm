{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/qcdong/codes/llamaIndex/app/tasks\n",
      "/home/qcdong/codes/llamaIndex/app/tasks/patents\n",
      "/home/qcdong/codes/llamaIndex/app/data/invention_processed.csv\n",
      "26\n",
      "['SEQ_ID_NO_18:WRRWWRRWWRWWRRWWRR']\n",
      "['SEQ_ID_NO_17:RWWRWWRRWWRR']\n",
      "['SEQ_ID_NO_19:RRVVRRVRRVVRRVVRVVRRVVRR']\n",
      "['SEQ_ID_NO_21:VRRVVRRVVRVVRRVVRRVRRVVRRVVRVVRRVVRR']\n",
      "['SEQ_ID_NO_20:RRWWRRWRRWWRRWWRWWRRWWRR']\n",
      "['SEQ_ID_NO_23:RRVVRRVRRVVRRVVRVVRRVVRRVRRVVRRVVRVVRRVVRR']\n",
      "['SEQ_ID_NO_22:VRRVWRRVVRVVRRWVRRVRRVWRRVVRVVRRWVRR']\n",
      "['SEQ_ID_NO_25:RVVRVVRRWVRRVRRVWRRVVRVVRRWVRRVRRVWRRVVRVVRRWRVV']\n",
      "['SEQ_ID_NO_24:RVVRVVRRVVRRVRRVVRRVVRVVRRVVRRVRRVVRRVVRVVRRVVRR']\n",
      "['SEQ_ID_NO_26:HHHHHH']\n",
      "['SEQ_ID_NO_2:IRRRRRRIRRRRRR']\n",
      "['SEQ_ID_NO_3:IRRRIRRIRRRIRRIRRRIRR']\n",
      "['SEQ_ID_NO_4:IRRIIRRIRRIIRRIRRIIRR']\n",
      "['SEQ_ID_NO_5:VWRWVRRVWRWVRRVWRWVRR']\n",
      "['SEQ_ID_NO_6:VWRWVRRVWRWVRR']\n",
      "['SEQ_ID_NO_7:VVRVVRRVVRVVRR']\n",
      "['SEQ_ID_NO_8:VVRVVRVVVRVVRVVVRVVRV']\n",
      "['SEQ_ID_NO_9:RSRVVRSWSRV']\n",
      "['SEQ_ID_NO_1:RRWVRRVRRVWRRVVRVVRRWVRR']\n",
      "['SEQ_ID_NO_10:RFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVR']\n",
      "['SEQ_ID_NO_12:KVVSSIIEIISSVVKVVSSIIEIISSVV']\n",
      "['SEQ_ID_NO_11:RRTYSRSRRTYSRSRRTYSR']\n",
      "['SEQ_ID_NO_14:VVRVVRRVVRVVRRVVRVVRR']\n",
      "['SEQ_ID_NO_13:KKTHTKTKKTHTKTKKTHTK']\n",
      "['SEQ_ID_NO_16:RVVRVVRRWVRR']\n",
      "['SEQ_ID_NO_15:RVVRVVRRVVRR']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "\n",
    "def shuffle_seq(seq_list):\n",
    "#用于随机排列seq\n",
    "    random.shuffle(seq_list)\n",
    "    return ''.join(seq_list)\n",
    "\n",
    "\n",
    "#对于sequence数量过多的patent，将其按一定间隔分段输入prompt\n",
    "def cut_prompts(df,total_length=1000,max_num=30,rand_num=5):\n",
    "    #设置最大prompt长度和最大seq数量，防止prompt占用过多token\n",
    "    prompts=[]\n",
    "    seq_set=[]\n",
    "    len_now=0\n",
    "    num_now=0\n",
    "    for index,row in df.iterrows():\n",
    "        new_str='{}:{}'.format(row['new_seq_id'],row['peptide sequence'])\n",
    "        #有一项不满足就新增prompt,并将一组seq随机排列多次\n",
    "        if len_now+len(new_str) > total_length or num_now+1 >max_num:\n",
    "            prompts.append([shuffle_seq(seq_set) for i in range(rand_num)])\n",
    "            seq_set=[]\n",
    "            len_now=0\n",
    "            num_now=0\n",
    "\n",
    "        len_now+=len(new_str)\n",
    "        num_now+=1\n",
    "        seq_set.append(new_str)\n",
    "        \n",
    "    prompts.append([shuffle_seq(seq_set) for i in range(rand_num)])\n",
    "    return prompts\n",
    "\n",
    "print(Path('.').absolute().parent)\n",
    "print(Path('.').parent.absolute())\n",
    "print((Path('.').absolute().parent.parent / 'data' / 'invention_processed.csv'))\n",
    "df_seq=pd.read_csv((Path('.').absolute().parent.parent / 'data' / 'invention_processed.csv'))\n",
    "patent_kind='A1'  #B\n",
    "patent_num='20200277334'  #\"09169290\"  09273096 11684559  09265709  20220062139 10329336\n",
    "patent_country='US'\n",
    "df_unit=df_seq[df_seq['patent_no']==patent_country+patent_num+patent_kind]\n",
    "print(len(df_unit))\n",
    "prompts=cut_prompts(df_unit, max_num=1, rand_num=1)\n",
    "for p in prompts:\n",
    "    print(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'elastic_transport.ObjectApiResponse'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3699560/3464205633.py:20: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  result = es.search(index=INDEX,body=body, request_timeout=60)\n",
      "/tmp/ipykernel_3699560/3464205633.py:20: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  result = es.search(index=INDEX,body=body, request_timeout=60)\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "import json\n",
    "\n",
    "es = Elasticsearch(hosts=\"http://192.168.1.124:29200\")\n",
    "INDEX = \"patents\"\n",
    "\n",
    "body = {\n",
    "    \"query\":{\n",
    "        \"bool\":{\n",
    "            \"must\":[\n",
    "                {\"term\":{\"publication_doc_number\": patent_num}},\n",
    "                {\"term\":{\"publication_country\": patent_country}}, \n",
    "                {\"term\":{\"publication_kind\":patent_kind}}               \n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "result = es.search(index=INDEX,body=body, request_timeout=60)\n",
    "#从es中获取原文\n",
    "print(type(result))\n",
    "res_dic=dict(result)\n",
    "res_js=json.dumps(dict(result),indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['took', 'timed_out', '_shards', 'hits'])\n",
      "dict_keys(['total', 'max_score', 'hits'])\n",
      "dict_keys(['_index', '_type', '_id', '_score', '_ignored', '_source'])\n",
      "hits num 1\n",
      "soucre_keys = dict_keys(['patent_type', 'invention', 'application_country', 'application_doc_number', 'application_date', 'publication_country', 'publication_doc_number', 'publication_kind', 'publication_date', 'abstract', 'classifications', 'classifications_name', 'description', 'claim', 'peptide_sequence'])\n",
      "abstract = ['Disclosed herein are novel peptides that can comprise antimicrobial, antiviral, antifungal or antitumor activity when administered to a subject.']\n",
      "description_keys = dict_keys(['Reference', 'Summary', 'Description', 'Others'])\n",
      "\n",
      "summary  1 <class 'list'> 33946\n",
      "SUMMARY--Disclosed herein are peptides. Peptides disclosed herein can comprise a polypeptide sequence of Formula A, Formula B, Formula C, Formula D, Formula E, Formula F, Formula G, Formula H, Formula I, Formula J, Formula K, Formula L, Formula M, Formula N, or a salt of any of these; where: Formula A can be (AA1-AA2-AA3-AA4-AA5-AA6-AA7)n; where AA1 can be independently X, Ar, or Y; and AA2, AA3, AA4, AA5, AA6, and AA7 can be independently Y, U, $ or @; Formula B can be (AA1-AA2-AA3-AA4-AA5-AA6-\n",
      "\n",
      "description  18 <class 'list'>\n",
      "desc_lens = [283, 14598, 63655, 4884, 16364, 1129, 2743, 1093, 450, 1362, 1489, 5357, 4059, 5320, 2667, 4637, 935, 1411]\n",
      "SEQUENCE LISTING--The instant application contains a Sequence Listing which has been submitted electronically in ASCII format and is hereby incorporated by reference in its entirety. Said ASCII copy, created on Apr. 24, 2020, is named 48615-701_301_SL.txt and is 9,850 bytes in size.\n",
      "\n",
      "len(claim) = 28, len(claim[0]) = 4\n",
      "claim[0][\"claim_text\"][0][:500] = '1.-43. (canceled)'\n",
      "claim[-1][\"claim_text\"][-1][-200:] =  '173. A method of making a kit, comprising combining the pharmaceutical formulation of claim 44 with a container.'\n",
      "count = 0\n"
     ]
    }
   ],
   "source": [
    "print(res_dic.keys())\n",
    "print(res_dic['hits'].keys())\n",
    "print(res_dic['hits']['hits'][0].keys())\n",
    "print('hits num', len(res_dic['hits']['hits']))\n",
    "\n",
    "soucre_keys = res_dic['hits']['hits'][0]['_source'].keys()\n",
    "print(f'{soucre_keys = }')\n",
    "abstract = res_dic['hits']['hits'][0]['_source']['abstract']\n",
    "print(f'{abstract = }')\n",
    "# dict_keys(['Reference', 'Summary', 'Description', 'Others'])\n",
    "description_keys = res_dic['hits']['hits'][0]['_source']['description'].keys()\n",
    "print(f'{description_keys = }')\n",
    "\n",
    "summary=res_dic['hits']['hits'][0]['_source']['description']['Summary']\n",
    "print('\\nsummary ', len(summary), type(summary), len(summary[0]))\n",
    "print(summary[0][:500])\n",
    "description = res_dic['hits']['hits'][0]['_source']['description']['Description']\n",
    "print('\\ndescription ', len(description), type(description))\n",
    "desc_lens = []\n",
    "for descr in description:\n",
    "    desc_lens.append(len(descr))\n",
    "print(f'{desc_lens = }')\n",
    "print(description[0][:500])\n",
    "\n",
    "print()\n",
    "claim = res_dic['hits']['hits'][0]['_source'][\"claim\"]\n",
    "print(f'{len(claim) = }, {len(claim[0]) = }\\n{claim[0][\"claim_text\"][0][:500] = }')\n",
    "print(f'{claim[-1][\"claim_text\"][-1][-200:] =  }')\n",
    "max = 2\n",
    "count = 0\n",
    "for c in claim:\n",
    "    # print(f'{type(c[\"claim_text\"]) = }{len(c[\"claim_text\"]) = }')\n",
    "    for c_text in c[\"claim_text\"]:\n",
    "        if 'malassezia' in c_text.lower():\n",
    "            count += 1\n",
    "            if count > max:\n",
    "                break\n",
    "            print(c_text)\n",
    "print(f'{count = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153. The pharmaceutical formulation of claim 44, wherein the peptide, a metabolite thereof, or salt thereof exhibits antimicrobial activity against a Streptococcus agalactiae bacteria strain at a minimum inhibitory concentration that is at least two-fold lower than a minimum inhibitory concentration for an antimicrobial activity against a Streptococcus pneumoniae bacteria strain in vitro.\n",
      "155. The pharmaceutical formulation of claim 44, wherein the peptide, a metabolite thereof, or salt thereof exhibits antimicrobial activity against a vancomycin-resistant Enterococcus faecium bacteria strain at a minimum inhibitory concentration that is at least two-fold lower than a minimum inhibitory concentration for an antimicrobial activity against a vancomycin-sensitive Enterococcus faecium bacteria strain in vitro.\n",
      "156. The pharmaceutical formulation of claim 44, wherein the peptide or salt thereof comprises at least about 95% homology to the polypeptide of sequence Arg Arg Trp Val Arg Arg Val Arg Arg Val Trp Arg Arg Val Val Arg Val Val Arg Arg Trp Val Arg Arg (SEQ ID NO: 1).\n",
      "count = 3\n"
     ]
    }
   ],
   "source": [
    "max = 22\n",
    "count = 0\n",
    "claim_nums = ['153', '155', '156']\n",
    "for c in claim:\n",
    "    for c_text in c[\"claim_text\"]:\n",
    "        for c_num in claim_nums:\n",
    "            if c_num in c_text.lower():\n",
    "                count += 1\n",
    "                if count > max:\n",
    "                    break\n",
    "                print(c_text)\n",
    "print(f'{count = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "ic| 355982704.py:14 in <module>- len(claims_raw): 8748, len(description_raw): 132483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(claims) = 28\n",
      "claims[0][: 500] = 'claim 1.-43. (canceled)'\n",
      "claims[-1][-200:] = 'claim 173. A method of making a kit, comprising combining the pharmaceutical formulation of claim 44 with a container.'\n",
      "claims_real[-200:] = '44 in a container.\\nclaim 172. The kit of claim 171, further comprising a syringe.\\nclaim 173. A method of making a kit, comprising combining the pharmaceutical formulation of claim 44 with a container.'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from icecream import ic\n",
    "ic.configureOutput(includeContext=True, argToStringFunction=str)\n",
    "ic.lineWrapWidth = 120\n",
    "\n",
    "from llama_index.core import Document\n",
    "\n",
    "claims=['claim '+' '.join(dic['claim_text']) for dic in res_dic['hits']['hits'][0]['_source'][\"claim\"]]\n",
    "print(f'{len(claims) = }')\n",
    "print(f'{claims[0][: 500] = }')\n",
    "print(f'{claims[-1][-200:] = }')\n",
    "claims_raw='The claim information:\\n'+'\\n'.join(claims)\n",
    "description_raw='\\nThe description information:\\n'+'\\n'.join(res_dic['hits']['hits'][0]['_source'][\"description\"][\"Description\"])\n",
    "ic(len(claims_raw), len(description_raw))\n",
    "#文本预处理(方便之后的关键词匹配)\n",
    "claims1=re.sub('\\u2003',' ',claims_raw)\n",
    "claims_real=re.sub('SEQ ID N[oO][.: ]+','SEQ_ID_NO_',claims1)\n",
    "description1=re.sub('\\u2003',' ',description_raw)\n",
    "description_real=re.sub('SEQ ID N[oO][.: ]+','SEQ_ID_NO_',description1)\n",
    "print(f'{claims_real[-200:] = }')\n",
    "#claim和description存储在不同的document中\n",
    "doc_claim = Document(text=claims_real)\n",
    "doc_des = Document(text=description_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-11 09:34:51.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_hf_llm\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1muse_8bit = 1\u001b[0m\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.86s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from llama_index.core import Settings\n",
    "\n",
    "import torch\n",
    "from llama_index.core import PromptTemplate, Settings\n",
    "from llama_index.llms.huggingface.base import HuggingFaceLLM\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from loguru import logger\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "BOS_TOKEN = \"<s>\"\n",
    "EOS_TOKEN = \"</s>\"\n",
    "INST_BEGIN = \"[INST] \"\n",
    "INST_END = \" [/INST]\"\n",
    "\n",
    "# def completion_to_prompt(completion):\n",
    "#     return f\"<s>[INST] {completion} [/INST]\"\n",
    "\n",
    "\n",
    "# def messages_to_prompt(messages):\n",
    "#     prompt = BOS_TOKEN\n",
    "#     for message in messages:\n",
    "#         if message.role == \"user\":\n",
    "#             prompt += f\"{INST_BEGIN}{message.content}{INST_END}\"\n",
    "#         elif message.role == \"assistant\":\n",
    "#             prompt += f\"{message.content}{EOS_TOKEN}\"\n",
    "\n",
    "#     return prompt\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an AI assistant that answers questions in a friendly manner, based on the given source documents. Here are some rules you always follow:\n",
    "- Generate human readable output, avoid creating output with gibberish text.\n",
    "- Generate only the requested output, don't include any other language before or after the requested output.\n",
    "- Never say thank you, that you are happy to help, that you are an AI agent, etc. Just answer directly.\n",
    "- Generate professional language typically used in business documents in North America.\n",
    "- Never generate offensive or foul language.\n",
    "\"\"\"\n",
    "\n",
    "query_wrapper_prompt = PromptTemplate(\n",
    "    \"[INST]<<SYS>>\\n\" + SYSTEM_PROMPT + \"<</SYS>>\\n\\n{query_str}[/INST] \"\n",
    ")\n",
    "\n",
    "def load_hf_llm(\n",
    "    model_path=\"/mnt/nas1/models/llama/pretrained_weights/llama2-7b-chat-hf\",\n",
    "):\n",
    "    use_8bit = 1\n",
    "    quantization_config_4bit = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        # bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    "    quantization_config_8bit = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "    )\n",
    "    if use_8bit:\n",
    "        quantization_config = quantization_config_8bit\n",
    "    else:\n",
    "        quantization_config = quantization_config_4bit\n",
    "    logger.info(f\"{use_8bit = }\")\n",
    "    llm = HuggingFaceLLM(\n",
    "        model_name=model_path,\n",
    "        tokenizer_name=model_path,\n",
    "        context_window=4096,\n",
    "        max_new_tokens=2048,\n",
    "        # generate_kwargs={\"temperature\": 0.01, \"do_sample\": True, 'pad_token_id': 2},\n",
    "        generate_kwargs={\"do_sample\": False},\n",
    "        # generate_kwargs={\"temperature\": 0.01, \"top_k\": 50, \"top_p\": 0.95},\n",
    "        query_wrapper_prompt=query_wrapper_prompt,\n",
    "        device_map=\"auto\",\n",
    "        model_kwargs={\n",
    "            \"quantization_config\": quantization_config,\n",
    "            \"torch_dtype\": torch.float16,\n",
    "        },\n",
    "    )\n",
    "    # print(llm.get_memory_footprint())\n",
    "    return llm\n",
    "\n",
    "\n",
    "# define embed model\n",
    "os.environ[\"LLAMA_INDEX_CACHE_DIR\"] = \"/mnt/nas1/models/llama_index_cache\"\n",
    "Settings.embed_model = \"local:/mnt/nas1/models/BAAI/bge-small-en-v1.5\"\n",
    "Settings.llm = load_hf_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = Settings.llm.complete(\"Hello! Can you tell me a poem about cats and dogs?\")\n",
    "# print(response.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import (\n",
    "    HierarchicalNodeParser,\n",
    "    SentenceSplitter,\n",
    ")\n",
    "# parse nodes\n",
    "# parser = SentenceSplitter(chunk_size=256,chunk_overlap=8) #控制每个切片的长度以及相邻切片的重叠长度\n",
    "# nodes = parser.get_nodes_from_documents([doc_claim,doc_des])\n",
    "\n",
    "node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=[1024, 512, 256])\n",
    "nodes = node_parser.get_nodes_from_documents([doc_claim,doc_des]) #documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8734 132473\n",
      "The claim information:\n",
      "claim 1.-43. (canceled)\n",
      "claim 44. A pharmaceutical formulation comprising:a. \n",
      "\n",
      "44 in a container.\n",
      "claim 172. The kit of claim 171, further comprising a syringe.\n",
      "claim 173. A method of making a kit, comprising combining the pharmaceutical formulation of claim 44 with a container.\n",
      "\n",
      "len(nodes) = 430\n",
      "len(node.text) = 2669, c4a7c168-1fdf-40d2-a48b-195200e057f0\n",
      "len(node.text) = 4341, 1b753d35-037b-4813-94d4-48b5a7b12ab1\n",
      "len(node.text) = 1790, dc8620c0-b1a0-4100-bda0-f044396afcb2\n",
      "len(node.text) = 1138, e2d96306-48fb-46f4-b85a-068b650f377a\n",
      "len(node.text) = 1530, 41337420-185f-49bd-b360-c9bcae572c77\n",
      "...\n",
      "len(node.text) = 795\n",
      "len(node.text) = 804\n",
      "len(node.text) = 904\n",
      "len(node.text) = 190\n",
      "len(node.text) = 285\n",
      "\n",
      "The claim information:\n",
      "claim 1.-43. (canceled)\n",
      "claim 44. A pharmaceutical formulation comprising:a. a peptide or salt thereof comprising from about 70% to about 100% homology to a polypeptide of seque\n"
     ]
    }
   ],
   "source": [
    "print(len(doc_claim.text), len(doc_des.text))\n",
    "print(doc_claim.text[:100])\n",
    "print()\n",
    "print(doc_claim.text[-200:])\n",
    "print()\n",
    "print(f'{len(nodes) = }')\n",
    "for node in nodes[:5]:\n",
    "    print(f'{len(node.text) = }, {node.id_}')\n",
    "print('...')\n",
    "for node in nodes[-5:]:\n",
    "   print(f'{len(node.text) = }')\n",
    "print()\n",
    "print(nodes[0].text[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define storage context\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core.storage import StorageContext\n",
    "\n",
    "docstore = SimpleDocumentStore()\n",
    "# insert nodes into docstore\n",
    "docstore.add_documents(nodes)\n",
    "\n",
    "# define storage context (will include vector store by default too)\n",
    "storage_context = StorageContext.from_defaults(docstore=docstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(nodes) = 430\n",
      "len(leaf_nodes) = 258\n",
      "len(mid_nodes) = 123\n",
      "len(root_nodes) = 49\n",
      "       leaf_nodes_length\n",
      "count         258.000000\n",
      "mean          566.968992\n",
      "std           325.916557\n",
      "min            46.000000\n",
      "25%           310.000000\n",
      "50%           572.000000\n",
      "75%           795.000000\n",
      "max          1313.000000\n",
      "       mid_nodes_length\n",
      "count        123.000000\n",
      "mean        1162.544715\n",
      "std          638.462436\n",
      "min           47.000000\n",
      "25%          674.000000\n",
      "50%         1213.000000\n",
      "75%         1570.500000\n",
      "max         2423.000000\n"
     ]
    }
   ],
   "source": [
    "#层次索引需要载入底层的叶子节点\n",
    "from llama_index.core.node_parser import get_leaf_nodes, get_root_nodes, get_deeper_nodes\n",
    "from pandas import DataFrame\n",
    "\n",
    "print(f'{len(nodes) = }')\n",
    "leaf_nodes = get_leaf_nodes(nodes)\n",
    "print(f'{len(leaf_nodes) = }')\n",
    "mid_nodes = get_deeper_nodes(nodes)\n",
    "print(f'{len(mid_nodes) = }')\n",
    "root_nodes = get_root_nodes(nodes)\n",
    "print(f'{len(root_nodes) = }')\n",
    "\n",
    "leaf_nodes_lens = [len(node.text) for node in leaf_nodes]\n",
    "df = DataFrame(leaf_nodes_lens, columns=['leaf_nodes_length'])\n",
    "print(df.describe())\n",
    "mid_nodes_lens = [len(node.text) for node in mid_nodes]\n",
    "df = DataFrame(mid_nodes_lens, columns=['mid_nodes_length'])\n",
    "print(df.describe())\n",
    "## among the first 5 nodes, 3 is in the root nodes.\n",
    "# for node, root in zip(nodes[:5], root_nodes[:5]):\n",
    "#     print(f'{len(node.text) = }, {node.id_}, {node.text[:100]}')\n",
    "#     print(f'{len(root.text) = }, {root.id_}, {root.text[:100]}')\n",
    "#     print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load index into vector index\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# 利用叶子节点计算相似度，并关联到对应的父节点\n",
    "base_index = VectorStoreIndex(\n",
    "    leaf_nodes,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import QueryBundle\n",
    "from llama_index.core import QueryBundle\n",
    "\n",
    "# import NodeWithScore\n",
    "from llama_index.core.schema import NodeWithScore, BaseNode\n",
    "\n",
    "# Retrievers\n",
    "from llama_index.core.retrievers import (\n",
    "    BaseRetriever,\n",
    "    VectorIndexRetriever,\n",
    ")\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize \n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"自定义的混合索引类：相似度索引+关键词索引\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        vector_retriever_large: VectorIndexRetriever,\n",
    "        max_num_keyword_nodes=3,\n",
    "        keywords=[]\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever #头部相似度节点\n",
    "        self._vector_retriever_large = vector_retriever_large #更大范围的头部相似度节点(用于关键词搜索)\n",
    "\n",
    "        self.keywords=keywords #索引依据的关键词\n",
    "        self.max_num_keyword_nodes=max_num_keyword_nodes #设置最大关键词节点数量\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        #利用两个不同的参数的retriever进行retrieve\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        vector_nodes_large = self._vector_retriever_large.retrieve(query_bundle)\n",
    "\n",
    "        #确保集合中的节点id对应的相似度由大到小排列\n",
    "        vector_ids = {n.node_id for n in sorted(vector_nodes,key=lambda node: node.score,reverse=True)} \n",
    "        # vector_ids_large = {n.node_id for n in vector_nodes_large}\n",
    "\n",
    "        #对于更大范围的相似度索引结果，取出其中含有关键词的节点\n",
    "        keyword_ids = []\n",
    "        for n in sorted(vector_nodes_large,key=lambda node: node.score,reverse=True):\n",
    "            for k in self.keywords:\n",
    "                if(k in word_tokenize(n.get_content())):\n",
    "                    #判断关键词是否在文章片段的分词结果中\n",
    "                    keyword_ids.append(n.node_id)\n",
    "                    break\n",
    "\n",
    "        combined_dict = {n.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node_id: n for n in vector_nodes_large if n.node_id in keyword_ids })\n",
    "\n",
    "        #合并两组节点\n",
    "        if(keyword_ids==[]):\n",
    "            #不含有关键词的情况下照常进行相似度索引\n",
    "            retrieve_ids = vector_ids\n",
    "        else:\n",
    "            keyword_ids_top=set(keyword_ids[:self.max_num_keyword_nodes]) #取相似度最高的几个关键词节点\n",
    "            vector_ids_unique=vector_ids-keyword_ids_top  #top相似度集合中独有的节点\n",
    "            retrieve_ids=keyword_ids_top #关键词集合和top相似度集合共有的节点+关键词集合中独有的节点\n",
    "            add_num=len(vector_ids)-len(keyword_ids_top)\n",
    "            retrieve_ids=set(list(vector_ids_unique)[:add_num]).union(retrieve_ids) #额外添加部分top相似度集合中独有的节点\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SEQ_ID_NO_18:WRRWWRRWWRWWRRWWRR']\n",
      "['SEQ_ID_NO_17:RWWRWWRRWWRR']\n",
      "['SEQ_ID_NO_19:RRVVRRVRRVVRRVVRVVRRVVRR']\n",
      "['SEQ_ID_NO_21:VRRVVRRVVRVVRRVVRRVRRVVRRVVRVVRRVVRR']\n",
      "['SEQ_ID_NO_20:RRWWRRWRRWWRRWWRWWRRWWRR']\n",
      "['SEQ_ID_NO_23:RRVVRRVRRVVRRVVRVVRRVVRRVRRVVRRVVRVVRRVVRR']\n",
      "['SEQ_ID_NO_22:VRRVWRRVVRVVRRWVRRVRRVWRRVVRVVRRWVRR']\n",
      "['SEQ_ID_NO_25:RVVRVVRRWVRRVRRVWRRVVRVVRRWVRRVRRVWRRVVRVVRRWRVV']\n",
      "['SEQ_ID_NO_24:RVVRVVRRVVRRVRRVVRRVVRVVRRVVRRVRRVVRRVVRVVRRVVRR']\n",
      "['SEQ_ID_NO_26:HHHHHH']\n",
      "['SEQ_ID_NO_2:IRRRRRRIRRRRRR']\n",
      "['SEQ_ID_NO_3:IRRRIRRIRRRIRRIRRRIRR']\n",
      "['SEQ_ID_NO_4:IRRIIRRIRRIIRRIRRIIRR']\n",
      "['SEQ_ID_NO_5:VWRWVRRVWRWVRRVWRWVRR']\n",
      "['SEQ_ID_NO_6:VWRWVRRVWRWVRR']\n",
      "['SEQ_ID_NO_7:VVRVVRRVVRVVRR']\n",
      "['SEQ_ID_NO_8:VVRVVRVVVRVVRVVVRVVRV']\n",
      "['SEQ_ID_NO_9:RSRVVRSWSRV']\n",
      "['SEQ_ID_NO_1:RRWVRRVRRVWRRVVRVVRRWVRR']\n",
      "['SEQ_ID_NO_10:RFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVR']\n",
      "['SEQ_ID_NO_12:KVVSSIIEIISSVVKVVSSIIEIISSVV']\n",
      "['SEQ_ID_NO_11:RRTYSRSRRTYSRSRRTYSR']\n",
      "['SEQ_ID_NO_14:VVRVVRRVVRVVRRVVRVVRR']\n",
      "['SEQ_ID_NO_13:KKTHTKTKKTHTKTKKTHTK']\n",
      "['SEQ_ID_NO_16:RVVRVVRRWVRR']\n",
      "['SEQ_ID_NO_15:RVVRVVRRVVRR']\n",
      "\n",
      "['SEQ_ID_NO_7:VVRVVRRVVRVVRR']\n",
      "['SEQ_ID_NO_7:VVRVVRRVVRVVRR']\n"
     ]
    }
   ],
   "source": [
    "for p in prompts:\n",
    "    print(p)\n",
    "print('')\n",
    "print(prompts[15])\n",
    "print(prompts[15][0].split('\\n'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompts[15][0] = 'SEQ_ID_NO_7:VVRVVRRVVRVVRR'\n",
      "keywords = ['SEQ_ID_NO_7']\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.retrievers.auto_merging_retriever import AutoMergingRetriever\n",
    "base_retriever = base_index.as_retriever(similarity_top_k=6)\n",
    "large_retriever = base_index.as_retriever(similarity_top_k=len(leaf_nodes)//2)\n",
    "\n",
    "print(f'{prompts[15][0] = }')\n",
    "keywords=[i.split(':')[0] for i in prompts[15][0].split('\\n')]\n",
    "print(f'{keywords = }')\n",
    "custom_retriever = CustomRetriever(base_retriever, large_retriever,max_num_keyword_nodes=3,keywords=[i.split(':')[0] for i in prompts[15][0].split('\\n')])  \n",
    "#创建混合索引实例\n",
    "retriever = AutoMergingRetriever(custom_retriever, storage_context, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "custom_query_engine = RetrieverQueryEngine.from_args(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#修改默认的prompt内容\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "qa_prompt_tmpl_str = (\n",
    "    \"\"\"You are a biologist and patent expert. \n",
    "    You will be provided with some contents from a patent and will be asked to answer specific questions related to the patent. \n",
    "    Please answer the question only using the provided contents and do not make up the answer with prior knowledge.\"\"\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Content is :\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the provided content, please answer the query:\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "\n",
    "custom_query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Merging 2 nodes into parent node.\n",
      "> Parent node id: a609603b-43eb-47cd-9611-4c72c782577e.\n",
      "> Parent node text: To this solution will be added an exemplary peptide of SEQ_ID_NO_1, SEQ_ID_NO_2, SEQ_ID_NO_3, SEQ...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/qcdong/anaconda3/envs/llaIdx/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Yes, the patent is describing agents (e.g. molecules) for inhibiting Malassezia fungus. The patent specifically mentions Malassezia in the \"Background of the Invention\" section, and the \"Summary of the Invention\" section highlights the need for effective treatments against Malassezia infections. The \"Claims\" section also includes claims related to the use of the described peptides for inhibiting Malassezia growth.\n",
       "Reason: The patent explicitly mentions Malassezia in multiple sections, including the \"Background of the Invention\" and the \"Summary of the Invention,\" and includes claims related to the use of the described peptides for inhibiting Malassezia growth. This indicates that the patent is indeed describing agents for inhibiting Malassezia fungus.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "question1=\"Is this patent describing agents (e.g. molecules) for inhibiting Malassezia fungus? Such applications should be claimed in the “Claims” of the patent. Please make sure the patent is specific about inhibition of Malassezia. Finally, plesae answer 'Yes' or 'No' first, then explain the reason in next line.\" #Please focuse on the given context and don't use prior knowledge.\n",
    "response = custom_query_engine.query(question1)\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llaIdx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
