{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "['SEQ_ID_NO_18:WRRWWRRWWRWWRRWWRR']\n",
      "['SEQ_ID_NO_17:RWWRWWRRWWRR']\n",
      "['SEQ_ID_NO_19:RRVVRRVRRVVRRVVRVVRRVVRR']\n",
      "['SEQ_ID_NO_21:VRRVVRRVVRVVRRVVRRVRRVVRRVVRVVRRVVRR']\n",
      "['SEQ_ID_NO_20:RRWWRRWRRWWRRWWRWWRRWWRR']\n",
      "['SEQ_ID_NO_23:RRVVRRVRRVVRRVVRVVRRVVRRVRRVVRRVVRVVRRVVRR']\n",
      "['SEQ_ID_NO_22:VRRVWRRVVRVVRRWVRRVRRVWRRVVRVVRRWVRR']\n",
      "['SEQ_ID_NO_25:RVVRVVRRWVRRVRRVWRRVVRVVRRWVRRVRRVWRRVVRVVRRWRVV']\n",
      "['SEQ_ID_NO_24:RVVRVVRRVVRRVRRVVRRVVRVVRRVVRRVRRVVRRVVRVVRRVVRR']\n",
      "['SEQ_ID_NO_26:HHHHHH']\n",
      "['SEQ_ID_NO_2:IRRRRRRIRRRRRR']\n",
      "['SEQ_ID_NO_3:IRRRIRRIRRRIRRIRRRIRR']\n",
      "['SEQ_ID_NO_4:IRRIIRRIRRIIRRIRRIIRR']\n",
      "['SEQ_ID_NO_5:VWRWVRRVWRWVRRVWRWVRR']\n",
      "['SEQ_ID_NO_6:VWRWVRRVWRWVRR']\n",
      "['SEQ_ID_NO_7:VVRVVRRVVRVVRR']\n",
      "['SEQ_ID_NO_8:VVRVVRVVVRVVRVVVRVVRV']\n",
      "['SEQ_ID_NO_9:RSRVVRSWSRV']\n",
      "['SEQ_ID_NO_1:RRWVRRVRRVWRRVVRVVRRWVRR']\n",
      "['SEQ_ID_NO_10:RFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVR']\n",
      "['SEQ_ID_NO_12:KVVSSIIEIISSVVKVVSSIIEIISSVV']\n",
      "['SEQ_ID_NO_11:RRTYSRSRRTYSRSRRTYSR']\n",
      "['SEQ_ID_NO_14:VVRVVRRVVRVVRRVVRVVRR']\n",
      "['SEQ_ID_NO_13:KKTHTKTKKTHTKTKKTHTK']\n",
      "['SEQ_ID_NO_16:RVVRVVRRWVRR']\n",
      "['SEQ_ID_NO_15:RVVRVVRRVVRR']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_seq=pd.read_csv('./data/invention_processed.csv')\n",
    "patent_kind='A1'  #B\n",
    "patent_num='20200277334'  #\"09169290\"  09273096 11684559  09265709  20220062139 10329336\n",
    "patent_country='US'\n",
    "\n",
    "import random\n",
    "\n",
    "def shuffle_seq(seq_list):\n",
    "#用于随机排列seq\n",
    "    random.shuffle(seq_list)\n",
    "    return ''.join(seq_list)\n",
    "\n",
    "#对于sequence数量过多的patent，将其按一定间隔分段输入prompt\n",
    "def cut_prompts(df,total_length=1000,max_num=30,rand_num=5):\n",
    "    #设置最大prompt长度和最大seq数量，防止prompt占用过多token\n",
    "    prompts=[]\n",
    "    seq_set=[]\n",
    "    len_now=0\n",
    "    num_now=0\n",
    "    for index,row in df.iterrows():\n",
    "        new_str='{}:{}'.format(row['new_seq_id'],row['peptide sequence'])\n",
    "        #有一项不满足就新增prompt,并将一组seq随机排列多次\n",
    "        if len_now+len(new_str) > total_length or num_now+1 >max_num:\n",
    "            prompts.append([shuffle_seq(seq_set) for i in range(rand_num)])\n",
    "            seq_set=[]\n",
    "            len_now=0\n",
    "            num_now=0\n",
    "\n",
    "        len_now+=len(new_str)\n",
    "        num_now+=1\n",
    "        seq_set.append(new_str)\n",
    "        \n",
    "    prompts.append([shuffle_seq(seq_set) for i in range(rand_num)])\n",
    "    return prompts\n",
    "\n",
    "df_unit=df_seq[df_seq['patent_no']==patent_country+patent_num+patent_kind]\n",
    "print(len(df_unit))\n",
    "prompts=cut_prompts(df_unit, max_num=1, rand_num=1)\n",
    "for p in prompts:\n",
    "    print(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'elastic_transport.ObjectApiResponse'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26757/3464205633.py:20: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  result = es.search(index=INDEX,body=body, request_timeout=60)\n",
      "/tmp/ipykernel_26757/3464205633.py:20: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  result = es.search(index=INDEX,body=body, request_timeout=60)\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "import json\n",
    "\n",
    "es = Elasticsearch(hosts=\"http://192.168.1.124:29200\")\n",
    "INDEX = \"patents\"\n",
    "\n",
    "body = {\n",
    "    \"query\":{\n",
    "        \"bool\":{\n",
    "            \"must\":[\n",
    "                {\"term\":{\"publication_doc_number\": patent_num}},\n",
    "                {\"term\":{\"publication_country\": patent_country}}, \n",
    "                {\"term\":{\"publication_kind\":patent_kind}}               \n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "result = es.search(index=INDEX,body=body, request_timeout=60)\n",
    "#从es中获取原文\n",
    "print(type(result))\n",
    "res_dic=dict(result)\n",
    "res_js=json.dumps(dict(result),indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['took', 'timed_out', '_shards', 'hits'])\n",
      "dict_keys(['total', 'max_score', 'hits'])\n",
      "dict_keys(['_index', '_type', '_id', '_score', '_ignored', '_source'])\n",
      "hits num 1\n",
      "dict_keys(['patent_type', 'invention', 'application_country', 'application_doc_number', 'application_date', 'publication_country', 'publication_doc_number', 'publication_kind', 'publication_date', 'abstract', 'classifications', 'classifications_name', 'description', 'claim', 'peptide_sequence'])\n",
      "['Disclosed herein are novel peptides that can comprise antimicrobial, antiviral, antifungal or antitumor activity when administered to a subject.']\n",
      "dict_keys(['Reference', 'Summary', 'Description', 'Others'])\n",
      "\n",
      "summary  1 <class 'list'> 33946\n",
      "SUMMARY--Disclosed herein are peptides. Peptides disclosed herein can comprise a polypeptide sequence of Formula A, Formula B, Formula C, Formula D, Formula E, Formula F, Formula G, Formula H, Formula I, Formula J, Formula K, Formula L, Formula M, Formula N, or a salt of any of these; where: Formula A can be (AA1-AA2-AA3-AA4-AA5-AA6-AA7)n; where AA1 can be independently X, Ar, or Y; and AA2, AA3, AA4, AA5, AA6, and AA7 can be independently Y, U, $ or @; Formula B can be (AA1-AA2-AA3-AA4-AA5-AA6-\n",
      "\n",
      "description  18 <class 'list'>\n",
      "283\n",
      "14598\n",
      "63655\n",
      "4884\n",
      "16364\n",
      "1129\n",
      "2743\n",
      "1093\n",
      "450\n",
      "1362\n",
      "1489\n",
      "5357\n",
      "4059\n",
      "5320\n",
      "2667\n",
      "4637\n",
      "935\n",
      "1411\n",
      "SEQUENCE LISTING--The instant application contains a Sequence Listing which has been submitted electronically in ASCII format and is hereby incorporated by reference in its entirety. Said ASCII copy, created on Apr. 24, 2020, is named 48615-701_301_SL.txt and is 9,850 bytes in size.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(res_dic.keys())\n",
    "print(res_dic['hits'].keys())\n",
    "print(res_dic['hits']['hits'][0].keys())\n",
    "print('hits num', len(res_dic['hits']['hits']))\n",
    "\n",
    "print(res_dic['hits']['hits'][0]['_source'].keys())\n",
    "print(res_dic['hits']['hits'][0]['_source']['abstract'])\n",
    "\n",
    "# dict_keys(['Reference', 'Summary', 'Description', 'Others'])\n",
    "print(res_dic['hits']['hits'][0]['_source']['description'].keys())\n",
    "\n",
    "summary=res_dic['hits']['hits'][0]['_source']['description']['Summary']\n",
    "print('\\nsummary ', len(summary), type(summary), len(summary[0]))\n",
    "print(summary[0][:500])\n",
    "description = res_dic['hits']['hits'][0]['_source']['description']['Description']\n",
    "print('\\ndescription ', len(description), type(description))\n",
    "for descr in description:\n",
    "    print(len(descr))\n",
    "print(description[0][:500])\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| 2849017804.py:9 in <module>- len(res_dic['hits']['hits'][0]['_source'][\"claim\"]): 28\n",
      "ic| 2849017804.py:12 in <module>- len(claims_raw): 8748, len(description_raw): 132483\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from icecream import ic\n",
    "ic.configureOutput(includeContext=True, argToStringFunction=str)\n",
    "ic.lineWrapWidth = 120\n",
    "\n",
    "from llama_index.core import Document\n",
    "\n",
    "claims=['claim '+' '.join(dic['claim_text']) for dic in res_dic['hits']['hits'][0]['_source'][\"claim\"]]\n",
    "ic(len(res_dic['hits']['hits'][0]['_source'][\"claim\"]))\n",
    "claims_raw='The claim information:\\n'+'\\n'.join(claims)\n",
    "description_raw='\\nThe description information:\\n'+'\\n'.join(res_dic['hits']['hits'][0]['_source'][\"description\"][\"Description\"])\n",
    "ic(len(claims_raw), len(description_raw))\n",
    "#文本预处理(方便之后的关键词匹配)\n",
    "claims1=re.sub('\\u2003',' ',claims_raw)\n",
    "claims_real=re.sub('SEQ ID N[oO][.: ]+','SEQ_ID_NO_',claims1)\n",
    "description1=re.sub('\\u2003',' ',description_raw)\n",
    "description_real=re.sub('SEQ ID N[oO][.: ]+','SEQ_ID_NO_',description1)\n",
    "\n",
    "#claim和description存储在不同的document中\n",
    "doc_claim = Document(text=claims_real)\n",
    "doc_des = Document(text=description_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.73s/it]\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from llama_index.core import Settings\n",
    "\n",
    "import torch\n",
    "from llama_index.core import PromptTemplate, Settings\n",
    "from llama_index.llms.huggingface.base import HuggingFaceLLM\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from loguru import logger\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "BOS_TOKEN = \"<s>\"\n",
    "EOS_TOKEN = \"</s>\"\n",
    "INST_BEGIN = \"[INST] \"\n",
    "INST_END = \" [/INST]\"\n",
    "\n",
    "def completion_to_prompt(completion):\n",
    "    return f\"<s>[INST] {completion} [/INST]\"\n",
    "\n",
    "\n",
    "def messages_to_prompt(messages):\n",
    "    prompt = BOS_TOKEN\n",
    "    for message in messages:\n",
    "        if message.role == \"user\":\n",
    "            prompt += f\"{INST_BEGIN}{message.content}{INST_END}\"\n",
    "        elif message.role == \"assistant\":\n",
    "            prompt += f\"{message.content}{EOS_TOKEN}\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def load_hf_llm(\n",
    "    model_path=\"/mnt/nas1/models/MaziyarPanahi/Calme-7B-Instruct-v0.2\",\n",
    "):\n",
    "    use_8bit = 1\n",
    "    quantization_config_4bit = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        # bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    "    quantization_config_8bit = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "    )\n",
    "    if use_8bit:\n",
    "        quantization_config = quantization_config_8bit\n",
    "    else:\n",
    "        quantization_config = quantization_config_4bit\n",
    "    logger.info(f\"{use_8bit = }\")\n",
    "    llm = HuggingFaceLLM(\n",
    "        model_name=model_path,\n",
    "        tokenizer_name=model_path,\n",
    "        context_window=3900,\n",
    "        max_new_tokens=256,\n",
    "        # generate_kwargs={\"temperature\": 0.01, \"do_sample\": True, 'pad_token_id': 2},\n",
    "        generate_kwargs={\"do_sample\": False, \"pad_token_id\": 2},\n",
    "        # generate_kwargs={\"temperature\": 0.01, \"top_k\": 50, \"top_p\": 0.95},\n",
    "        messages_to_prompt=messages_to_prompt,\n",
    "        completion_to_prompt=completion_to_prompt,\n",
    "        device_map=\"auto\",\n",
    "        model_kwargs={\n",
    "            \"quantization_config\": quantization_config,\n",
    "        },\n",
    "    )\n",
    "    # print(llm.get_memory_footprint())\n",
    "    return llm\n",
    "\n",
    "\n",
    "# define embed model\n",
    "os.environ[\"LLAMA_INDEX_CACHE_DIR\"] = \"/mnt/nas1/models/llama_index_cache\"\n",
    "Settings.embed_model = \"local:/mnt/nas1/models/BAAI/bge-small-en-v1.5\"\n",
    "Settings.llm = load_hf_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import (\n",
    "    HierarchicalNodeParser,\n",
    "    SentenceSplitter,\n",
    ")\n",
    "# parse nodes\n",
    "# parser = SentenceSplitter(chunk_size=256,chunk_overlap=8) #控制每个切片的长度以及相邻切片的重叠长度\n",
    "# nodes = parser.get_nodes_from_documents([doc_claim,doc_des])\n",
    "\n",
    "node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=[1024, 512, 256])\n",
    "nodes = node_parser.get_nodes_from_documents([doc_claim,doc_des]) #documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8734 132473 The claim information:\n",
      "claim 1.-43. (canceled)\n",
      "claim 44. A pharmaceutical formulation comprising:a.  \n",
      " d of making a kit, comprising combining the pharmaceutical formulation of claim 44 with a container.\n",
      "len(nodes) = 430\n",
      "len(node.text) = 2669, f1ab6dc0-8a91-4075-8f4b-3cee23808f2d\n",
      "len(node.text) = 4341, deb74c82-2f40-4ee1-a054-216fb7f2806a\n",
      "len(node.text) = 1790, 674857b0-9577-49f5-a81a-5832307280a7\n",
      "len(node.text) = 1138, df1ba9bb-0ec5-4d8d-8e87-d57d4b49c0d4\n",
      "len(node.text) = 1530, 70afe37c-3610-4f04-9b1d-f5b21a5e9a39\n",
      "...\n",
      "795\n",
      "804\n",
      "904\n",
      "190\n",
      "285\n",
      "The claim information:\n",
      "claim 1.-43. (canceled)\n",
      "claim 44. A pharmaceutical formulation comprising:a. a peptide or salt thereof comprising from about 70% to about 100% homology to a polypeptide of sequence: (SEQ_ID_NO_15)Arg-Val-Val-Arg-Val-Val-Arg-Arg-Val-Val-Arg-Arg; (SEQ_ID_NO_16)Arg-Val-Val-Arg-Val-Val-Arg-Arg-Trp-Val-Arg-Arg; (SEQ_ID_NO_17)Arg-Trp-Trp-Arg-Trp-Trp-Arg-Arg-Trp-Trp-Arg-Arg; (SEQ_ID_NO_18)Trp-Arg-Arg-Trp-Trp-Arg-Arg-Trp-Trp-Arg-Trp-Trp- Arg-Arg-Trp-Trp-Arg-Arg; (SEQ_ID_NO_19)Arg-Arg-Val-Val-Arg-Arg-Val-Arg-Arg-Val-Val-Arg- Arg-Val-Val-Arg-Val-Val-Arg-Arg-Val-Val-Arg-Arg; (SEQ_ID_NO_1)Arg-Arg-Trp-Val-Arg-Arg-Val-Arg-Arg-Val-Trp-Arg- Arg-Val-Val-Arg-Val-Val-Arg-Arg-Trp-Val-Arg-Arg; (SEQ_ID_NO_20)Arg-Arg-Trp-Trp-Arg-Arg-Trp-Arg-Arg-Trp-Trp-Arg- Arg-Trp-Trp-Arg-Trp-Trp-Arg-Arg-Trp-Trp-Arg-Arg; (SEQ_ID_NO_21)Val-Arg-Arg-Val-Val-Arg-Arg-Val-Val-Arg-Val-Val- Arg-Arg-Val-Val-Arg-Arg-Val-Arg-Arg-Val-Val-Arg- Arg-Val-Val-Arg-Val-Val-Arg-Arg-Val-Val-Arg-Arg; (SEQ_ID_NO_22)Val-Arg-Arg-Val-Trp-Arg-Arg-Val-Val-Arg-Val-Val- Arg-Arg-Trp-Val-Arg-Arg-Val-Arg-Arg-Val-Trp-Arg- Arg-Val-Val-Arg-Val-Val-Arg-Arg-Trp-Val-Arg-Arg; (SEQ_ID_NO_23)Arg-Arg-Val-Val-Arg-Arg-Val-Arg-Arg-Val-Val-Arg- Arg-Val-Val-Arg-Val-Val-Arg-Arg-Val-Val-Arg-Arg- Val-Arg-Arg-Val-Val-Arg-Arg-Val-Val-Arg-Val-Val- Arg-Arg-Val-Val-Arg-Arg; (SEQ_ID_NO_24)Arg-Val-Val-Arg-Val-Val-Arg-Arg-Val-Val-Arg-Arg- Val-Arg-Arg-Val-Val-Arg-Arg-Val-Val-Arg-Val-Val- Arg-Arg-Val-Val-Arg-Arg-Val-Arg-Arg-Val-Val-Arg- Arg-Val-Val-Arg-Val-Val-Arg-Arg-Val-Val-Arg-Arg;or (SEQ_ID_NO_25)Arg-Val-Val-Arg-Val-Val-Arg-Arg-Trp-Val-Arg-Arg- Val-Arg-Arg-Val-Trp-Arg-Arg-Val-Val-Arg-Val-Val- Arg-Arg-Trp-Val-Arg-Arg-Val-Arg-Arg-Val-Trp-Arg- Arg-Val-Val-Arg-Val-Val-Arg-Arg-Trp-Arg-Val-Val;b. at least one of: an excipient, a diluent, or a carrier; wherein the pharmaceutical formulation is in unit dose form, wherein the peptide does not comprise 3 or more contiguous arginine or lysine residues; wherein the peptide is not a cyclic peptide; and wherein at least one of the following applies:(i) the peptide, a metabolite thereof, or salt thereof exhibits antimicrobial activity against a bacteria with a minimum inhibitory concentration ranging from about 0.1 μg/mL to about 100 μg/mL in vitro;(ii) the peptide, a metabolite thereof, or salt thereof exhibits antifungal activity against a fungus with a minimum inhibitory concentration ranging from about 0.1 μg/mL to about 100 μg/mL in vitro;(iii) the peptide, a metabolite thereof, or salt thereof exhibits antiviral activity against a virus with a minimum inhibitory concentration ranging from about 0.1 μg/mL to about 100 μg/mL in vitro; or(iv) the peptide,\n"
     ]
    }
   ],
   "source": [
    "print(len(doc_claim.text), len(doc_des.text), doc_claim.text[:100], '\\n', doc_claim.text[-100:])\n",
    "print(f'{len(nodes) = }')\n",
    "for node in nodes[:5]:\n",
    "    print(f'{len(node.text) = }, {node.id_}')\n",
    "print('...')\n",
    "for node in nodes[-5:]:\n",
    "    print(len(node.text))\n",
    "print(nodes[0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define storage context\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core.storage import StorageContext\n",
    "\n",
    "docstore = SimpleDocumentStore()\n",
    "# insert nodes into docstore\n",
    "docstore.add_documents(nodes)\n",
    "\n",
    "# define storage context (will include vector store by default too)\n",
    "storage_context = StorageContext.from_defaults(docstore=docstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(nodes) = 430\n",
      "258\n",
      "49\n",
      "len(mid_nodes) = 123\n",
      "len(node.text) = 2669, f1ab6dc0-8a91-4075-8f4b-3cee23808f2d, The claim information:\n",
      "claim 1.-43. (canceled)\n",
      "claim 44. A pharmaceutical formulation comprising:a. \n",
      "len(root.text) = 2669, f1ab6dc0-8a91-4075-8f4b-3cee23808f2d, The claim information:\n",
      "claim 1.-43. (canceled)\n",
      "claim 44. A pharmaceutical formulation comprising:a. \n",
      "\n",
      "len(node.text) = 4341, deb74c82-2f40-4ee1-a054-216fb7f2806a, 1 μg/mL to about 100 μg/mL in vitro; or(iv) the peptide, a metabolite thereof, or salt thereof exhib\n",
      "len(root.text) = 4341, deb74c82-2f40-4ee1-a054-216fb7f2806a, 1 μg/mL to about 100 μg/mL in vitro; or(iv) the peptide, a metabolite thereof, or salt thereof exhib\n",
      "\n",
      "len(node.text) = 1790, 674857b0-9577-49f5-a81a-5832307280a7, claim 167. The pharmaceutical formulation of claim 44, that is in the form of a tablet, a liquid, a \n",
      "len(root.text) = 1790, 674857b0-9577-49f5-a81a-5832307280a7, claim 167. The pharmaceutical formulation of claim 44, that is in the form of a tablet, a liquid, a \n",
      "\n",
      "len(node.text) = 1138, df1ba9bb-0ec5-4d8d-8e87-d57d4b49c0d4, The claim information:\n",
      "claim 1.-43. (canceled)\n",
      "claim 44. A pharmaceutical formulation comprising:a. \n",
      "len(root.text) = 3596, 2997d0de-7fda-4da9-891e-5afad392118b, The description information:\n",
      "SEQUENCE LISTING--The instant application contains a Sequence Listing w\n",
      "\n",
      "len(node.text) = 1530, 70afe37c-3610-4f04-9b1d-f5b21a5e9a39, (SEQ_ID_NO_23)Arg-Arg-Val-Val-Arg-Arg-Val-Arg-Arg-Val-Val-Arg- Arg-Val-Val-Arg-Val-Val-Arg-Arg-Val-V\n",
      "len(root.text) = 4506, cba2f3d9-4bb1-4db4-8558-16cb567f19de, A subject can be a patient. A subject can be an individual. In some instances, a subject, patient or\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#层次索引需要载入底层的叶子节点\n",
    "from llama_index.core.node_parser import get_leaf_nodes, get_root_nodes, get_deeper_nodes\n",
    "print(f'{len(nodes) = }')\n",
    "leaf_nodes = get_leaf_nodes(nodes)\n",
    "print(len(leaf_nodes))\n",
    "root_nodes = get_root_nodes(nodes)\n",
    "print(len(root_nodes))\n",
    "mid_nodes = get_deeper_nodes(nodes)\n",
    "print(f'{len(mid_nodes) = }')\n",
    "for node, root in zip(nodes[:5], root_nodes[:5]):\n",
    "    print(f'{len(node.text) = }, {node.id_}, {node.text[:100]}')\n",
    "    print(f'{len(root.text) = }, {root.id_}, {root.text[:100]}')\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load index into vector index\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# 利用叶子节点计算相似度，并关联到对应的父节点\n",
    "base_index = VectorStoreIndex(\n",
    "    leaf_nodes,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import QueryBundle\n",
    "from llama_index.core import QueryBundle\n",
    "\n",
    "# import NodeWithScore\n",
    "from llama_index.core.schema import NodeWithScore, BaseNode\n",
    "\n",
    "# Retrievers\n",
    "from llama_index.core.retrievers import (\n",
    "    BaseRetriever,\n",
    "    VectorIndexRetriever,\n",
    ")\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize \n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"自定义的混合索引类：相似度索引+关键词索引\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        vector_retriever_large: VectorIndexRetriever,\n",
    "        max_num_keyword_nodes=3,\n",
    "        keywords=[]\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever #头部相似度节点\n",
    "        self._vector_retriever_large = vector_retriever_large #更大范围的头部相似度节点(用于关键词搜索)\n",
    "\n",
    "        self.keywords=keywords #索引依据的关键词\n",
    "        self.max_num_keyword_nodes=max_num_keyword_nodes #设置最大关键词节点数量\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        #利用两个不同的参数的retriever进行retrieve\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        vector_nodes_large = self._vector_retriever_large.retrieve(query_bundle)\n",
    "\n",
    "        #确保集合中的节点id对应的相似度由大到小排列\n",
    "        vector_ids = {n.node_id for n in sorted(vector_nodes,key=lambda node: node.score,reverse=True)} \n",
    "        # vector_ids_large = {n.node_id for n in vector_nodes_large}\n",
    "\n",
    "        #对于更大范围的相似度索引结果，取出其中含有关键词的节点\n",
    "        keyword_ids = []\n",
    "        for n in sorted(vector_nodes_large,key=lambda node: node.score,reverse=True):\n",
    "            for k in self.keywords:\n",
    "                if(k in word_tokenize(n.get_content())):\n",
    "                    #判断关键词是否在文章片段的分词结果中\n",
    "                    keyword_ids.append(n.node_id)\n",
    "                    break\n",
    "\n",
    "        combined_dict = {n.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node_id: n for n in vector_nodes_large if n.node_id in keyword_ids })\n",
    "\n",
    "        #合并两组节点\n",
    "        if(keyword_ids==[]):\n",
    "            #不含有关键词的情况下照常进行相似度索引\n",
    "            retrieve_ids = vector_ids\n",
    "        else:\n",
    "            keyword_ids_top=set(keyword_ids[:self.max_num_keyword_nodes]) #取相似度最高的几个关键词节点\n",
    "            vector_ids_unique=vector_ids-keyword_ids_top  #top相似度集合中独有的节点\n",
    "            retrieve_ids=keyword_ids_top #关键词集合和top相似度集合共有的节点+关键词集合中独有的节点\n",
    "            add_num=len(vector_ids)-len(keyword_ids_top)\n",
    "            retrieve_ids=set(list(vector_ids_unique)[:add_num]).union(retrieve_ids) #额外添加部分top相似度集合中独有的节点\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SEQ_ID_NO_18:WRRWWRRWWRWWRRWWRR']\n",
      "['SEQ_ID_NO_17:RWWRWWRRWWRR']\n",
      "['SEQ_ID_NO_19:RRVVRRVRRVVRRVVRVVRRVVRR']\n",
      "['SEQ_ID_NO_21:VRRVVRRVVRVVRRVVRRVRRVVRRVVRVVRRVVRR']\n",
      "['SEQ_ID_NO_20:RRWWRRWRRWWRRWWRWWRRWWRR']\n",
      "['SEQ_ID_NO_23:RRVVRRVRRVVRRVVRVVRRVVRRVRRVVRRVVRVVRRVVRR']\n",
      "['SEQ_ID_NO_22:VRRVWRRVVRVVRRWVRRVRRVWRRVVRVVRRWVRR']\n",
      "['SEQ_ID_NO_25:RVVRVVRRWVRRVRRVWRRVVRVVRRWVRRVRRVWRRVVRVVRRWRVV']\n",
      "['SEQ_ID_NO_24:RVVRVVRRVVRRVRRVVRRVVRVVRRVVRRVRRVVRRVVRVVRRVVRR']\n",
      "['SEQ_ID_NO_26:HHHHHH']\n",
      "['SEQ_ID_NO_2:IRRRRRRIRRRRRR']\n",
      "['SEQ_ID_NO_3:IRRRIRRIRRRIRRIRRRIRR']\n",
      "['SEQ_ID_NO_4:IRRIIRRIRRIIRRIRRIIRR']\n",
      "['SEQ_ID_NO_5:VWRWVRRVWRWVRRVWRWVRR']\n",
      "['SEQ_ID_NO_6:VWRWVRRVWRWVRR']\n",
      "['SEQ_ID_NO_7:VVRVVRRVVRVVRR']\n",
      "['SEQ_ID_NO_8:VVRVVRVVVRVVRVVVRVVRV']\n",
      "['SEQ_ID_NO_9:RSRVVRSWSRV']\n",
      "['SEQ_ID_NO_1:RRWVRRVRRVWRRVVRVVRRWVRR']\n",
      "['SEQ_ID_NO_10:RFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVRRFVRRVR']\n",
      "['SEQ_ID_NO_12:KVVSSIIEIISSVVKVVSSIIEIISSVV']\n",
      "['SEQ_ID_NO_11:RRTYSRSRRTYSRSRRTYSR']\n",
      "['SEQ_ID_NO_14:VVRVVRRVVRVVRRVVRVVRR']\n",
      "['SEQ_ID_NO_13:KKTHTKTKKTHTKTKKTHTK']\n",
      "['SEQ_ID_NO_16:RVVRVVRRWVRR']\n",
      "['SEQ_ID_NO_15:RVVRVVRRVVRR']\n",
      "\n",
      "['SEQ_ID_NO_7:VVRVVRRVVRVVRR']\n",
      "['SEQ_ID_NO_7:VVRVVRRVVRVVRR']\n",
      "['SEQ_ID_NO_7']\n"
     ]
    }
   ],
   "source": [
    "for p in prompts:\n",
    "    print(p)\n",
    "print('')\n",
    "print(prompts[15])\n",
    "print(prompts[15][0].split('\\n'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompts[15][0] = 'SEQ_ID_NO_7:VVRVVRRVVRVVRR'\n",
      "['SEQ_ID_NO_7']\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from llama_index.core.retrievers.auto_merging_retriever import AutoMergingRetriever\n",
    "base_retriever = base_index.as_retriever(similarity_top_k=6)\n",
    "large_retriever = base_index.as_retriever(similarity_top_k=len(leaf_nodes)//2)\n",
    "\n",
    "print(f'{prompts[15][0] = }')\n",
    "keywords=[i.split(':')[0] for i in prompts[15][0].split('\\n')]\n",
    "print(keywords)\n",
    "custom_retriever = CustomRetriever(base_retriever, large_retriever,max_num_keyword_nodes=3,keywords=[i.split(':')[0] for i in prompts[15][0].split('\\n')])  \n",
    "#创建混合索引实例\n",
    "retriever = AutoMergingRetriever(custom_retriever, storage_context, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "custom_query_engine = RetrieverQueryEngine.from_args(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#修改默认的prompt内容\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "qa_prompt_tmpl_str = (\n",
    "    \"\"\"You are a biologist and patent expert. \n",
    "    You will be provided with some contents from a patent and will be asked to answer specific questions related to the patent. \n",
    "    Please answer the question only using the provided contents and do not make up the answer with prior knowledge.\"\"\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Content is :\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the provided content, please answer the query:\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "\n",
    "custom_query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Merging 2 nodes into parent node.\n",
      "> Parent node id: 490db001-1938-4455-8ba0-0a8befce6a6a.\n",
      "> Parent node text: To this solution will be added an exemplary peptide of SEQ_ID_NO_1, SEQ_ID_NO_2, SEQ_ID_NO_3, SEQ...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>NoINSTINST: The given patent focuses on antiviral, antifungal, parasitic, and other pathogen treatments through peptides, but it does not explicitly mention Malassezia or an application for inhibiting it. The \"Claims\" section, which often outlines specific applications, is also absent from the provided context.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "question1=\"Is this patent describing agents (e.g. molecules) for inhibiting Malassezia fungus? Such applications should be claimed in the “Claims” of the patent. Please make sure the patent is specific about inhibition of Malassezia. Finally, plesae answer 'Yes' or 'No' first, then explain the reason in next line.\" #Please focuse on the given context and don't use prior knowledge.\n",
    "response = custom_query_engine.query(question1)\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llaIdx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
